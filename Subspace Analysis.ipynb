{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "97d04bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../TransformerLens/\")\n",
    "\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification, CLIPVisionModelWithProjection, AutoProcessor\n",
    "from transformer_lens.HookedViT import HookedViT\n",
    "from transformer_lens.loading_from_pretrained import convert_vit_weights, convert_clip_weights\n",
    "import transformer_lens.utils as utils\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import shutil\n",
    "import random\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.contrib import itertools as tqdm_itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "aee96e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pretrain=\"imagenet\", patch_size=32, compositional=-1):\n",
    "    '''\n",
    "    :param pretrain: \"scracth\", \"imagenet\", \"clip\"\n",
    "    :param patch_size: = 16, 32\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    except AttributeError:  # if MPS is not available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "        \n",
    "    if compositional < 0:\n",
    "        train_str = \"256-256-256\"\n",
    "    else:\n",
    "        train_str = f\"{compositional}-{compositional}-{256-compositional}\"\n",
    "    \n",
    "    if pretrain == \"scratch\" or pretrain == \"imagenet\":\n",
    "        hf_model = ViTForImageClassification.from_pretrained(f\"google/vit-base-patch{patch_size}-224-in21k\", num_labels=2).to(device)\n",
    "        tl_model = HookedViT.from_pretrained(f\"google/vit-base-patch{patch_size}-224-in21k\").to(device)\n",
    "        image_processor = AutoImageProcessor.from_pretrained(f\"google/vit-base-patch{patch_size}-224-in21k\")\n",
    "\n",
    "    elif pretrain == \"clip\":\n",
    "        hf_model = CLIPVisionModelWithProjection.from_pretrained(f\"openai/clip-vit-base-patch{patch_size}\")\n",
    "        tl_model = HookedViT.from_pretrained(f\"openai/clip-vit-base-patch{patch_size}\", is_clip=True)\n",
    "        image_processor = AutoProcessor.from_pretrained(f\"openai/clip-vit-base-patch{patch_size}\")\n",
    "        \n",
    "        in_features = hf_model.visual_projection.in_features\n",
    "        hf_model.visual_projection = torch.nn.Linear(in_features, 2, bias=False).to(\"mps\")\n",
    "        tl_model.classifier_head.W = hf_model.visual_projection.weight\n",
    "        \n",
    "    model_path = glob.glob(f\"models/{pretrain}_vit{patch_size}/{train_str}*.pth\")[0]\n",
    "    model_file = torch.load(model_path, map_location=torch.device(device))\n",
    "    hf_model.load_state_dict(model_file)\n",
    "    \n",
    "    if pretrain == \"clip\":\n",
    "        state_dict = convert_clip_weights(hf_model, tl_model.cfg)\n",
    "    else:\n",
    "        state_dict = convert_vit_weights(hf_model, tl_model.cfg)\n",
    "    tl_model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    tl_model.eval()\n",
    "    return image_processor, tl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "6fe8be2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "def create_noise_image(o, im, split_channels=True):\n",
    "    if split_channels:\n",
    "        mu = [int(o.split(\"-\")[i].split(\"_\")[0].replace(\"mean\", \"\")) for i in range(1, 4)]\n",
    "    else:\n",
    "        mu = int(o.split(\"-\")[-1].split(\"_\")[0].replace(\"mean\", \"\"))\n",
    "    sigma = int(o.split(\"-\")[-1].split(\"_\")[1][:-4].replace(\"var\", \"\"))\n",
    "\n",
    "    data = im.getdata()\n",
    "\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        if item[0] == 255 and item[1] == 255 and item[2] == 255:\n",
    "            new_data.append(item)\n",
    "        else:\n",
    "            if split_channels: \n",
    "                noise = np.zeros(3, dtype=np.uint8)\n",
    "                for i in range(3):\n",
    "                    noise[i] = np.random.normal(\n",
    "                        loc=mu[i],\n",
    "                        scale=sigma,\n",
    "                        size=(1)\n",
    "                    ).clip(min=0, max=250).astype(np.uint8)\n",
    "            else:\n",
    "                noise = (\n",
    "                    np.random.normal(loc=mu, scale=sigma, size=(1))\n",
    "                    .clip(min=0, max=250)\n",
    "                    .astype(np.uint8)\n",
    "                )\n",
    "                noise = np.repeat(noise, 3, axis=0)\n",
    "            new_data.append(tuple(noise))\n",
    "\n",
    "    im.putdata(new_data)\n",
    "\n",
    "def create_particular_stimulus(\n",
    "    shape_1,\n",
    "    shape_2,\n",
    "    texture_1,\n",
    "    texture_2,\n",
    "    position_1,\n",
    "    position_2,\n",
    "    buffer_factor=8,\n",
    "    im_size=224,\n",
    "    patch_size=32,\n",
    "    split_channels=True\n",
    "):\n",
    "    # Shape_1 is an integer\n",
    "    # Texture_1 is a mean_var pair\n",
    "    # position_1 is an x, y pair\n",
    "    coords = np.linspace(\n",
    "        0, im_size, num=(im_size // patch_size), endpoint=False, dtype=int\n",
    "    )\n",
    "    possible_coords = list(itertools.product(coords, repeat=2))\n",
    "\n",
    "    x_1 = coords[position_1[0]]\n",
    "    y_1 = coords[position_1[1]]\n",
    "    idx_1 = possible_coords.index((x_1, y_1))\n",
    "    x_2 = coords[position_2[0]]\n",
    "    y_2 = coords[position_2[1]]\n",
    "    idx_2 = possible_coords.index((x_2, y_2))\n",
    "\n",
    "    path1 = f\"{shape_1}-{texture_1}.png\"\n",
    "    path2 = f\"{shape_2}-{texture_2}.png\"\n",
    "\n",
    "    im1 = Image.open(f\"stimuli/source/NOISE_RGB/{patch_size}/{path1}\").convert(\"RGB\")\n",
    "    im1 = im1.resize(\n",
    "        (\n",
    "            patch_size - (patch_size // buffer_factor),\n",
    "            patch_size - (patch_size // buffer_factor),\n",
    "        ),\n",
    "        Image.NEAREST,\n",
    "    )\n",
    "\n",
    "    im2 = Image.open(f\"stimuli/source/NOISE_RGB/{patch_size}/{path2}\").convert(\"RGB\")\n",
    "    im2 = im2.resize(\n",
    "        (\n",
    "            patch_size - (patch_size // buffer_factor),\n",
    "            patch_size - (patch_size // buffer_factor),\n",
    "        ),\n",
    "        Image.NEAREST,\n",
    "    )\n",
    "\n",
    "    # Sample noise\n",
    "    create_noise_image(path1, im1, split_channels=split_channels)\n",
    "    create_noise_image(path2, im2, split_channels=split_channels)\n",
    "\n",
    "    # Create blank image and paste objects\n",
    "    base = Image.new(\"RGB\", (im_size, im_size), (255, 255, 255))\n",
    "\n",
    "    box1 = [\n",
    "        coord + random.randint(0, patch_size // buffer_factor) for coord in [x_1, y_1]\n",
    "    ]\n",
    "    base.paste(im1, box=box1)\n",
    "\n",
    "    box2 = [\n",
    "        coord + random.randint(0, patch_size // buffer_factor) for coord in [x_2, y_2]\n",
    "    ]\n",
    "    base.paste(im2, box=box2)\n",
    "\n",
    "    return base\n",
    "\n",
    "def create_subspace_datasets(patch_size=32, mode=\"val\", analysis=\"texture\", split_channels=True, compositional=-1,\n",
    "                            distractor=False):\n",
    "    path_elements = [\"subspace\", f\"{analysis}_{patch_size}\"]\n",
    "    stub = \"stimuli/\"\n",
    "    num_patch = 224 // patch_size\n",
    "\n",
    "    if analysis == \"shape\":\n",
    "        other_feat_str = \"texture\"\n",
    "    else:\n",
    "        other_feat_str = \"shape\"\n",
    "    \n",
    "    for element in path_elements:\n",
    "        try:\n",
    "            os.mkdir(stub + element)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        stub += element + \"/\"\n",
    "    \n",
    "    if compositional > 0:\n",
    "        train_str = f\"trainsize_6400_{compositional}-{compositional}-{256 - compositional}\"\n",
    "    else:\n",
    "        train_str = \"trainsize_6400_256-256-256\"\n",
    "        \n",
    "    try:\n",
    "        os.mkdir(stub + train_str)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(stub + train_str + f\"/{mode}\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    all_ims = glob.glob(f\"stimuli/source/NOISE_RGB/{patch_size}/*.png\")\n",
    "    all_ims = [im.split(\"/\")[-1][:-4].split(\"-\") for im in all_ims]\n",
    "    all_ims = [[im[0], f\"{im[1]}-{im[2]}-{im[3]}\"] for im in all_ims]\n",
    "    shapes = set([im[0] for im in all_ims])\n",
    "    textures = set([im[1] for im in all_ims])\n",
    "    feature_dict = {\"shape\": sorted(list(shapes)), \"texture\": sorted(list(textures))}\n",
    "        \n",
    "    stim_dir = f\"stimuli/NOISE_RGB/aligned/N_{patch_size}/{train_str}\"\n",
    "    base_imfiles = glob.glob(f\"{stim_dir}/{mode}/different-{analysis}/*.png\")\n",
    "    stim_dict = pickle.load(open(f\"{stim_dir}/{mode}/datadict.pkl\", \"rb\"))\n",
    "    \n",
    "    for base in base_imfiles:\n",
    "        im = Image.open(base)\n",
    "        base_path = os.path.join(*base.split(\"/\")[-3:])\n",
    "        base_idx = base.split(\"/\")[-1][:-4]\n",
    "        datadict = {}\n",
    "        \n",
    "        base_dir = f\"{stub}{train_str}/{mode}/set_{base_idx}\"\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(base_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        same_stim = stim_dict[f\"{mode}/same/{base_idx}.png\"]\n",
    "        diff_stim = stim_dict[base_path]\n",
    "        datadict[\"base.png\"] = diff_stim.copy()\n",
    "        datadict[\"same.png\"] = same_stim.copy()\n",
    "        \n",
    "        try:\n",
    "            datadict[\"base.png\"].pop(\"sd-label\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            datadict[\"same.png\"].pop(\"sd-label\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        shutil.copy(base, f\"{base_dir}/base.png\")\n",
    "        shutil.copy(f\"{stim_dir}/{mode}/same/{base_idx}.png\", f\"{base_dir}/same.png\")\n",
    "        \n",
    "        if same_stim[f\"{analysis[0]}1\"] != diff_stim[f\"{analysis[0]}1\"]:  # Which object in the image is the edited one?\n",
    "            edited_idx = 1\n",
    "            not_edited_idx = 2\n",
    "        else:\n",
    "            edited_idx = 2\n",
    "            not_edited_idx = 1\n",
    "            \n",
    "        # For each texture/shape present in the \"different\" stimulus, create versions with every shape/texture\n",
    "        feature0 = diff_stim[f\"{analysis[0]}{edited_idx}\"]\n",
    "        feature1 = same_stim[f\"{analysis[0]}1\"]\n",
    "        \n",
    "        for feature, feature_str in zip([feature0, feature1], [f\"{analysis}0\", f\"{analysis}1\"]):\n",
    "            other_idx = 0\n",
    "            \n",
    "            for other_feat in feature_dict[other_feat_str]:\n",
    "                other_1 = diff_stim[f\"{other_feat_str[0]}{not_edited_idx}\"]\n",
    "                other_2 = other_feat\n",
    "                feat_1 = diff_stim[f\"{analysis[0]}{not_edited_idx}\"]\n",
    "                feat_2 = feature\n",
    "                position_1 = diff_stim[f\"pos{not_edited_idx}\"]\n",
    "                position_2 = diff_stim[f\"pos{edited_idx}\"]\n",
    "                \n",
    "                if analysis == \"texture\":\n",
    "                    dict_str = f\"{feature_str}_shape{other_idx}.png\"\n",
    "                else:\n",
    "                    dict_str = f\"{feature_str}_texture{other_idx}.png\"\n",
    "                    \n",
    "                datadict[dict_str] = {\"pos1\": position_1, f\"{analysis[0]}1\": feat_1, f\"{other_feat_str[0]}1\": other_1,\n",
    "                                      \"pos2\": position_2, f\"{analysis[0]}2\": feat_2, f\"{other_feat_str[0]}2\": other_2}\n",
    "\n",
    "                position_1 = [position_1 % num_patch, position_1 // num_patch]\n",
    "                position_2 = [position_2 % num_patch, position_2 // num_patch]\n",
    "\n",
    "                if analysis == \"texture\":\n",
    "                    im = create_particular_stimulus(other_1, other_2, feat_1, feat_2, position_1, position_2)\n",
    "                else:\n",
    "                    im = create_particular_stimulus(feat_1, feat_2, other_1, other_2, position_1, position_2)\n",
    "                    \n",
    "                im.save(f\"{base_dir}/{dict_str}\")\n",
    "                other_idx += 1\n",
    "        \n",
    "        if distractor:\n",
    "            if analysis == \"texture\":\n",
    "                distractor_feats = list(set(textures) - set([feature0, feature1]))\n",
    "            else:\n",
    "                distractor_feats = list(set(shapes) - set([feature0, feature1]))\n",
    "\n",
    "            np.random.shuffle(distractor_feats)\n",
    "            for d in range(len(distractor_feats)):\n",
    "                feature = distractor_feats[d]\n",
    "                #feature = np.random.choice(distractor_feats)\n",
    "                feature_str = f\"{analysis}{d}\"\n",
    "\n",
    "                other_idx = 0\n",
    "\n",
    "                for other_feat in feature_dict[other_feat_str]:\n",
    "                    other_1 = diff_stim[f\"{other_feat_str[0]}{not_edited_idx}\"]\n",
    "                    other_2 = other_feat\n",
    "                    feat_1 = diff_stim[f\"{analysis[0]}{not_edited_idx}\"]\n",
    "                    feat_2 = feature\n",
    "                    position_1 = diff_stim[f\"pos{not_edited_idx}\"]\n",
    "                    position_2 = diff_stim[f\"pos{edited_idx}\"]\n",
    "\n",
    "                    if analysis == \"texture\":\n",
    "                        dict_str = f\"{feature_str}_shape{other_idx}.png\"\n",
    "                    else:\n",
    "                        dict_str = f\"{feature_str}_texture{other_idx}.png\"\n",
    "\n",
    "                    datadict[dict_str] = {\"pos1\": position_1, f\"{analysis[0]}1\": feat_1, f\"{other_feat_str[0]}1\": other_1,\n",
    "                                          \"pos2\": position_2, f\"{analysis[0]}2\": feat_2, f\"{other_feat_str[0]}2\": other_2}\n",
    "\n",
    "                    position_1 = [position_1 % num_patch, position_1 // num_patch]\n",
    "                    position_2 = [position_2 % num_patch, position_2 // num_patch]\n",
    "\n",
    "                    if analysis == \"texture\":\n",
    "                        im = create_particular_stimulus(other_1, other_2, feat_1, feat_2, position_1, position_2)\n",
    "                    else:\n",
    "                        im = create_particular_stimulus(feat_1, feat_2, other_1, other_2, position_1, position_2)\n",
    "\n",
    "                    im.save(f\"{base_dir}/{dict_str}\")\n",
    "                    other_idx += 1\n",
    "                \n",
    "        with open(f\"{base_dir}/datadict.pkl\", \"wb\") as handle:\n",
    "            pickle.dump(datadict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "def create_counting_datasets(patch_size=32, buffer_factor=8, n=-1, k=9, n_per_class=100):\n",
    "    '''\n",
    "    :param n: number of unique tasks; by default, max number of unique objects\n",
    "    :param k: number of objs per im\n",
    "    '''\n",
    "    all_ims = glob.glob(f\"stimuli/source/NOISE_RGB/{patch_size}/*.png\")\n",
    "    if n < 0:\n",
    "        n = len(all_ims)\n",
    "    assert n <= len(all_ims)\n",
    "    \n",
    "    num_patch = 224 // patch_size\n",
    "    assert k <= num_patch**2\n",
    "    \n",
    "    coords = np.linspace(\n",
    "        0, 224, num=(224 // patch_size), endpoint=False, dtype=int\n",
    "    )\n",
    "    coords = list(itertools.product(coords, repeat=2))\n",
    "    \n",
    "    all_feats = [im.split(\"/\")[-1][:-4].split(\"-\") for im in all_ims]\n",
    "    all_feats = [[im[0], f\"{im[1]}-{im[2]}-{im[3]}\"] for im in all_feats]\n",
    "    all_shapes = set([im[0] for im in all_feats])\n",
    "    all_textures = set([im[1] for im in all_feats])\n",
    "    \n",
    "    object_ims_all = {}\n",
    "    for o in all_ims:\n",
    "        o_name = o.split(\"/\")[-1]\n",
    "        im = Image.open(o).convert(\"RGB\")\n",
    "        im = im.resize(\n",
    "            (\n",
    "                patch_size - (patch_size // buffer_factor),\n",
    "                patch_size - (patch_size // buffer_factor),\n",
    "            ),\n",
    "            Image.NEAREST,\n",
    "        )\n",
    "        object_ims_all[o_name] = im\n",
    "    \n",
    "    path_elements = [\"counting\", \"NOISE_RGB\", f\"N_{patch_size}\", f\"trainsize_{n_per_class}_{n}_{k}\"]\n",
    "    stub = \"stimuli/\"\n",
    "    \n",
    "    for element in path_elements:\n",
    "        try:\n",
    "            os.mkdir(stub + element)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        stub += element + \"/\"\n",
    "    \n",
    "    for mode in [\"train\", \"val\", \"test\"]:\n",
    "        try:\n",
    "            os.mkdir(stub + f\"/{mode}\")\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        for count_class in range(k + 1):\n",
    "            try:\n",
    "                os.mkdir(f\"{stub}/{mode}/{count_class}\")\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "    \n",
    "    # Choose objects to serve as \"task objects\" (i.e. objects to count)\n",
    "    task_ims = np.random.choice(all_ims, size=n)\n",
    "    \n",
    "    for mode in [\"train\", \"val\", \"test\"]:\n",
    "        print(mode)\n",
    "        im_idxs = {i: 0 for i in range(k + 1)}\n",
    "        data_dict = {}\n",
    "        \n",
    "        for task_idx in range(n):        \n",
    "            task_obj = task_ims[task_idx].split(\"/\")[-1]\n",
    "            feats = task_obj.split(\"-\")\n",
    "            task_obj_shape = feats[0]\n",
    "            task_obj_texture = f\"{feats[1]}-{feats[2]}-{feats[3][:-4]}\"\n",
    "            \n",
    "            diff_shapes = [d for d in all_shapes if d != task_obj_shape]\n",
    "            diff_textures = [d for d in all_textures if d != task_obj_texture]\n",
    "\n",
    "            for count_class in range(k + 1):\n",
    "                #task_dict[task_idx][f\"class_{count_class}\"] = {}\n",
    "\n",
    "                for _ in range(n_per_class):\n",
    "                    im_idx = im_idxs[count_class]\n",
    "                    datadict[f\"{mode}/{count_class}/{im_idx}.png\"] = {}\n",
    "                    imdict = datadict[f\"{mode}/{count_class}/{im_idx}.png\"]\n",
    "                    imdict[\"task_idx\"] = task_idx\n",
    "                    imdict[\"task_obj\"] = task_obj\n",
    "                    imdict[\"task_obj_shape\"] = task_obj_shape\n",
    "                    imdict[\"task_obj_texture\"] = task_obj_texture\n",
    "                    imdict[\"count_class\"] = count_class\n",
    "                    \n",
    "                    if count_class == k:\n",
    "                        n_same_shape = 0\n",
    "                        n_same_texture = 0\n",
    "                        n_diff = 0\n",
    "                    elif count_class == k - 1:\n",
    "                        i = np.random.choice(range(3))\n",
    "                        if i == 0:\n",
    "                            n_same_shape = 1\n",
    "                            n_same_texture = 0\n",
    "                            n_diff = 0\n",
    "                        elif i == 1:\n",
    "                            n_same_shape = 0\n",
    "                            n_same_texture = 1\n",
    "                            n_diff = 0\n",
    "                        else:\n",
    "                            n_same_shape = 0\n",
    "                            n_same_texture = 0\n",
    "                            n_diff = 1\n",
    "                    else:\n",
    "                        n_same_shape = np.random.choice(range(1, min(k - count_class, 4)))\n",
    "                        n_same_texture = np.random.choice(range(1, min(k - count_class - n_same_shape + 1, 4)))\n",
    "                        n_diff = k - count_class - n_same_shape - n_same_texture\n",
    "\n",
    "                    imdict[\"all_obj_pos\"] = np.random.choice(range(len(coords)), size=k, replace=False)\n",
    "                    \n",
    "                    for dictkey in [\"all_objs\", \"all_obj_shapes\", \"all_obj_textures\", \"task_obj_pos\", \n",
    "                             \"same_shape_pos\", \"same_texture_pos\", \"diff_pos\",\n",
    "                             \"task_obj_idx\", \"same_shape_idx\", \"same_texture_idx\", \"diff_idx\"]:\n",
    "                        imdict[dictkey] = []\n",
    "\n",
    "                    imdict[\"task_obj_pos\"] = imdict[\"all_obj_pos\"][:count_class]\n",
    "                    imdict[\"same_shape_pos\"] = imdict[\"all_obj_pos\"][count_class:count_class+n_same_shape]\n",
    "                    imdict[\"same_texture_pos\"] = imdict[\"all_obj_pos\"][count_class+n_same_shape:count_class+n_same_shape+n_same_texture]\n",
    "                    imdict[\"diff_pos\"] = imdict[\"all_obj_pos\"][count_class+n_same_shape+n_same_texture:]\n",
    "\n",
    "                    idx = 0\n",
    "                    for _ in range(count_class):\n",
    "                        imdict[\"all_objs\"].append(task_obj)\n",
    "                        imdict[\"all_obj_shapes\"].append(task_obj_shape)\n",
    "                        imdict[\"all_obj_textures\"].append(task_obj_texture)\n",
    "                        imdict[\"task_obj_idx\"].append(idx)\n",
    "                        idx += 1\n",
    "\n",
    "                    for _ in range(n_same_shape):\n",
    "                        s = task_obj_shape\n",
    "                        t = np.random.choice(diff_textures)\n",
    "                        obj = f\"{s}-{t}.png\"\n",
    "                        imdict[\"all_objs\"].append(obj)\n",
    "                        imdict[\"all_obj_shapes\"].append(s)\n",
    "                        imdict[\"all_obj_textures\"].append(t)\n",
    "                        imdict[\"same_shape_idx\"].append(idx)\n",
    "                        idx += 1\n",
    "\n",
    "                    for _ in range(n_same_texture):\n",
    "                        s = np.random.choice(diff_shapes)\n",
    "                        t = task_obj_texture\n",
    "                        obj = f\"{s}-{t}.png\"\n",
    "                        imdict[\"all_objs\"].append(obj)\n",
    "                        imdict[\"all_obj_shapes\"].append(s)\n",
    "                        imdict[\"all_obj_textures\"].append(t)\n",
    "                        imdict[\"same_texture_idx\"].append(idx)\n",
    "                        idx += 1\n",
    "\n",
    "                    for _ in range(n_diff):\n",
    "                        s = np.random.choice(diff_shapes)\n",
    "                        t = np.random.choice(diff_textures)\n",
    "                        obj = f\"{s}-{t}.png\"\n",
    "                        imdict[\"all_objs\"].append(obj)\n",
    "                        imdict[\"all_obj_shapes\"].append(s)\n",
    "                        imdict[\"all_obj_textures\"].append(t)\n",
    "                        imdict[\"diff_idx\"].append(idx)\n",
    "                        idx += 1\n",
    "\n",
    "                    # Create image\n",
    "                    base = Image.new(\"RGB\", (224, 224), (255, 255, 255))\n",
    "\n",
    "                    for o in range(k):\n",
    "                        obj = imdict[\"all_objs\"][o]\n",
    "                        im = object_ims_all[obj].copy()\n",
    "                        create_noise_image(obj, im, split_channels=True)\n",
    "\n",
    "                        pos = imdict[\"all_obj_pos\"][o]\n",
    "                        coord = coords[pos]\n",
    "                        box = [coord[i] + random.randint(0, patch_size // buffer_factor) for i in range(2)]\n",
    "\n",
    "                        base.paste(im, box=box)\n",
    "\n",
    "                    base.save(f\"{stub}/{mode}/{count_class}/{im_idx}.png\", quality=100) \n",
    "                    im_idxs[count_class] += 1\n",
    "            \n",
    "        with open(f\"{stub}/{mode}/datadict.pkl\", \"wb\") as handle:\n",
    "            pickle.dump(datadict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "create_counting_datasets(n=10, n_per_class=100, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "86da77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(set_idx, pretrain=\"imagenet\", mode=\"texture\", patch_size=32, device=\"mps\", random=False,\n",
    "           distractor=False, distractor_mode=\"add\", data_mode=\"val\", restricted=True, compositional=-1):\n",
    "    '''\n",
    "    Runs a single linear subspace experiment (i.e. on a single test set).\n",
    "    '''\n",
    "    \n",
    "    def preprocess(im):\n",
    "        if pretrain == \"clip\":\n",
    "            return image_processor(images=im, return_tensors='pt')[\"pixel_values\"].to(device)\n",
    "        else:\n",
    "            return image_processor.preprocess(im, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    \n",
    "    if random:\n",
    "        random_str = \"_random\"\n",
    "    else:\n",
    "        random_str = \"\"\n",
    "        \n",
    "    if compositional > 0:\n",
    "        data_str = f\"trainsize_6400_{compositional}-{compositional}-{256 - compositional}\"\n",
    "    else:\n",
    "        data_str = \"trainsize_6400_256-256-256\"\n",
    "    \n",
    "    num_tokens = (224 // patch_size)**2 + 1\n",
    "    \n",
    "    torch.set_grad_enabled(False)\n",
    "    image_processor, model = load_model(pretrain=pretrain, patch_size=patch_size)\n",
    "    \n",
    "    set_dir = f\"stimuli/subspace/{mode}_{patch_size}/{data_str}/{data_mode}/set_{set_idx}\"\n",
    "    \n",
    "    base = preprocess(np.array(Image.open(os.path.join(set_dir, \"base.png\"))))\n",
    "    same = preprocess(np.array(Image.open(os.path.join(set_dir, \"same.png\"))))\n",
    "\n",
    "    base_logits, base_cache = model.run_with_cache(base, remove_batch_dim=True)\n",
    "    if base_logits[0, 0] < base_logits[0, 1]:\n",
    "        return\n",
    "\n",
    "    same_logits, same_cache = model.run_with_cache(same, remove_batch_dim=True)\n",
    "    if same_logits[0, 0] > same_logits[0, 1]:\n",
    "        return\n",
    "    \n",
    "    base_logits_diff = base_logits[0, 1] - base_logits[0, 0]\n",
    "    same_logits_diff = same_logits[0, 1] - same_logits[0, 0]\n",
    "\n",
    "    # Images containing the original two different textures/shapes\n",
    "    if distractor and distractor_mode == \"subtract\":\n",
    "        base_feat = [np.array(Image.open(b), dtype=np.float32) for b in glob.glob(f\"{set_dir}/{mode}2_*.png\")]\n",
    "        distractor_str = f\"_{distractor_mode}{distractor}\"\n",
    "    else:\n",
    "        base_feat = [np.array(Image.open(b), dtype=np.float32) for b in glob.glob(f\"{set_dir}/{mode}0_*.png\")]\n",
    "        distractor_str = \"\"\n",
    "        \n",
    "    base_feat = preprocess(base_feat)\n",
    "    \n",
    "    # Images containing the same two textures\n",
    "    if distractor and distractor_mode == \"add\":\n",
    "        same_feat = [np.array(Image.open(s), dtype=np.float32) for s in glob.glob(f\"{set_dir}/{mode}2_*.png\")]\n",
    "        distractor_str = f\"_{distractor_mode}2\"\n",
    "    else:\n",
    "        same_feat = [np.array(Image.open(s), dtype=np.float32) for s in glob.glob(f\"{set_dir}/{mode}1_*.png\")]\n",
    "        distractor_str = \"\"\n",
    "        \n",
    "    same_feat = preprocess(same_feat)\n",
    "\n",
    "    def get_mean_cache(ims):\n",
    "        mean_cache = torch.zeros((model.cfg.n_layers, num_tokens, model.cfg.d_model), device=device)\n",
    "        _, cache = model.run_with_cache(ims)\n",
    "        \n",
    "        for layer in range(model.cfg.n_layers):\n",
    "            activations = cache[utils.get_act_name(\"resid_post\", layer)]\n",
    "            mean = torch.mean(activations, dim=0)\n",
    "            mean_cache[layer, :, :] = mean\n",
    "        \n",
    "        return mean_cache\n",
    "    \n",
    "    same_feat_vecs = get_mean_cache(same_feat)\n",
    "    base_feat_vecs = get_mean_cache(base_feat)\n",
    "    \n",
    "    if random:\n",
    "        random_feat_vecs = torch.zeros(base_feat_vecs.shape, device=device)\n",
    "        \n",
    "        for layer in range(base_feat_vecs.shape[0]):\n",
    "            for tok in range(base_feat_vecs.shape[1]):\n",
    "                random_vec = torch.from_numpy(np.random.normal(loc=0, scale=1, size=base_feat_vecs.shape[2]).astype(np.float32)).to(device)\n",
    "                random_vec /= torch.linalg.norm(random_vec)\n",
    "                random_vec *= torch.linalg.norm(base_feat_vecs[layer, tok, :])\n",
    "                random_feat_vecs[layer, tok, :] = random_vec\n",
    "                \n",
    "        base2same_feat_vecs = same_feat_vecs - random_feat_vecs\n",
    "    else:\n",
    "        base2same_feat_vecs = same_feat_vecs - base_feat_vecs  # Convert base texture to same texture\n",
    "    \n",
    "    def patch_fn(\n",
    "        target_residual_component: Float[torch.Tensor, \"batch tok d_model\"],\n",
    "        hook,\n",
    "        tok,\n",
    "        diff_vec,\n",
    "    ):\n",
    "        target_residual_component[:, tok, :] += diff_vec\n",
    "        return target_residual_component\n",
    "    \n",
    "    logit_diffs = torch.zeros((model.cfg.n_layers, num_tokens), device=device)\n",
    "    \n",
    "    if restricted:\n",
    "        stim_dict = pickle.load(open(os.path.join(set_dir, \"datadict.pkl\"), \"rb\"))\n",
    "        \n",
    "        if stim_dict[\"base.png\"][\"t1\"] != stim_dict[\"same.png\"][\"t1\"]:  # Which object in the image is the edited one?\n",
    "            edited_pos = stim_dict[\"base.png\"][\"pos1\"] + 1\n",
    "            not_edited_pos = stim_dict[\"base.png\"][\"pos2\"] + 1\n",
    "        else:\n",
    "            edited_pos = stim_dict[\"base.png\"][\"pos2\"] + 1\n",
    "            not_edited_pos = stim_dict[\"base.png\"][\"pos1\"] + 1\n",
    "        \n",
    "        iterator = tqdm_itertools.product(range(model.cfg.n_layers), [0, edited_pos, not_edited_pos])\n",
    "        restricted_str = \"_restricted\"\n",
    "    else:\n",
    "        iterator = tqdm_itertools.product(range(model.cfg.n_layers), range(num_tokens))\n",
    "        restricted_str = \"\"\n",
    "    \n",
    "    for layer, tok in iterator:\n",
    "        hook_fn = partial(patch_fn, tok=tok, diff_vec=base2same_feat_vecs[layer, tok])\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            base,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_post\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logits_diff = patched_logits[0, 1] - patched_logits[0, 0]\n",
    "        logit_diffs[layer, tok] = 2*(patched_logits_diff - base_logits_diff) / (same_logits_diff - base_logits_diff) - 1\n",
    "\n",
    "    torch.save(logit_diffs, f\"{set_dir}/logit_diffs_{pretrain}{restricted_str}{distractor_str}{random_str}.pt\", \n",
    "               pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return logit_diffs\n",
    "\n",
    "\n",
    "def subspace_score(pretrain=\"imagenet\", mode=\"texture\", patch_size=32, sets=None, n=10, device=\"mps\", \n",
    "                   plot=True, random=False, distractor=False, distractor_mode=\"add\", data_mode=\"val\",\n",
    "                   restricted=True, compositional=-1):\n",
    "    pretrain_label = {\"clip\": \"CLIP-Pretrained\", \"imagenet\": \"ImageNet-Pretrained\", \"scratch\": \"From Scratch\"}\n",
    "    \n",
    "    if distractor:\n",
    "        random = False\n",
    "        distractor_str = f\"_{distractor_mode}2\"\n",
    "    else:\n",
    "        distractor_str = \"\"\n",
    "    \n",
    "    if random:\n",
    "        random_str = \"_random\"\n",
    "    else:\n",
    "        random_str = \"\"\n",
    "        \n",
    "    if restricted:\n",
    "        restricted_str = \"_restricted\"\n",
    "    else:\n",
    "        restricted_str = \"\"\n",
    "        \n",
    "    if compositional > 0:\n",
    "        data_str = f\"trainsize_6400_{compositional}-{compositional}-{256 - compositional}\"\n",
    "    else:\n",
    "        data_str = \"trainsize_6400_256-256-256\"\n",
    "    \n",
    "    if sets is None:\n",
    "        existing_sets = [int(i.split(\"/\")[-1].split(\"_\")[-1]) for i in glob.glob(f\"stimuli/subspace/{mode}_{patch_size}/{data_str}/{data_mode}/*\")]\n",
    "        sets = np.random.choice(existing_sets, size=min(n, len(existing_sets)))\n",
    "    elif sets == \"all\":\n",
    "        sets = []\n",
    "        for set_idx in range(3200):\n",
    "            if os.path.isfile(f\"stimuli/subspace/{mode}_{patch_size}/{data_str}/{data_mode}/set_{set_idx}/logit_diffs_{pretrain}{restricted_str}{distractor_str}{random_str}.pt\"):\n",
    "                sets.append(set_idx)\n",
    "\n",
    "    #print(sets)\n",
    "    results = {s: {\"max_logit_diff\": None, \"max_logit_diff_idx\": None, \"logit_diffs\": None} for s in sets}\n",
    "    all_max_logit_diff = []\n",
    "    all_max_logit_diff_idx = []\n",
    "    all_logit_diff = []\n",
    "    all_edited_pos = []\n",
    "    all_not_edited_pos = []\n",
    "    \n",
    "    for set_idx in tqdm.tqdm(sets):\n",
    "        set_dir = f\"stimuli/subspace/{mode}_{patch_size}/{data_str}/{data_mode}/set_{set_idx}\"\n",
    "        stim_dict = pickle.load(open(f\"{set_dir}/datadict.pkl\", \"rb\"))\n",
    "        \n",
    "        try:\n",
    "            logit_diffs = torch.load(open(f\"{set_dir}/logit_diffs_{pretrain}{restricted_str}{distractor_str}{random_str}.pt\", \"rb\"))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Running experiment for set {set_idx}...\")\n",
    "            logit_diffs = run_exp(set_idx, pretrain=pretrain, mode=mode, patch_size=patch_size, device=device, \n",
    "                                  random=random, distractor=distractor, distractor_mode=\"add\", data_mode=data_mode, \n",
    "                                  restricted=restricted, compositional=compositional)\n",
    "            \n",
    "        if logit_diffs is None:\n",
    "            continue\n",
    "            \n",
    "        if stim_dict[\"base.png\"][f\"{mode[0]}1\"] != stim_dict[\"same.png\"][f\"{mode[0]}1\"]:  # Which object in the image is the edited one?\n",
    "            edited_pos = stim_dict[\"base.png\"][\"pos1\"] + 1\n",
    "            not_edited_pos = stim_dict[\"base.png\"][\"pos2\"] + 1\n",
    "        else:\n",
    "            edited_pos = stim_dict[\"base.png\"][\"pos2\"] + 1\n",
    "            not_edited_pos = stim_dict[\"base.png\"][\"pos1\"] + 1\n",
    "            \n",
    "        all_edited_pos.append(edited_pos)\n",
    "        all_not_edited_pos.append(not_edited_pos)\n",
    "\n",
    "        max_idx = torch.argmax(logit_diffs[:, edited_pos])\n",
    "        max_val = logit_diffs[max_idx, edited_pos]\n",
    "        \n",
    "        results[set_idx][\"max_logit_diff\"] = max_val.item()\n",
    "        results[set_idx][\"max_logit_diff_idx\"] = max_idx.item()\n",
    "        results[set_idx][\"logit_diffs\"] = logit_diffs.cpu()\n",
    "        \n",
    "        all_max_logit_diff.append(max_val.item())\n",
    "        all_max_logit_diff_idx.append(max_idx.item())\n",
    "        all_logit_diff.append(logit_diffs.cpu())\n",
    "    \n",
    "    print(f\"{pretrain_label[pretrain]} ViT-B/{patch_size} subspace score: {np.mean(all_max_logit_diff)} (variance={np.var(all_max_logit_diff)})\")\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(12, 9))\n",
    "        sns.histplot(data=all_max_logit_diff_idx, stat=\"count\", discrete=True, ax=ax[0][0])\n",
    "        ax[0][0].set_xticks(range(12))\n",
    "        ax[0][0].set_title(\"Model layer where max logit difference occurs across base images\")\n",
    "\n",
    "        sns.histplot(data=all_max_logit_diff, stat=\"count\", bins=40, ax=ax[0][1])\n",
    "        ax[0][1].set_title(\"Max logit difference across base images\")\n",
    "        \n",
    "        all_new_logit_diff = []\n",
    "        for s in range(len(all_logit_diff)):\n",
    "            new_logit_diff = torch.zeros((all_logit_diff[0].shape[0], 3))\n",
    "            new_logit_diff[:, 0] = all_logit_diff[s][:, 0]\n",
    "            new_logit_diff[:, 1] = torch.select(all_logit_diff[s], 1, all_edited_pos[s])\n",
    "            new_logit_diff[:, 2] = torch.select(all_logit_diff[s], 1, all_not_edited_pos[s])\n",
    "            all_new_logit_diff.append(new_logit_diff)\n",
    "        \n",
    "        avg_logit_diff = torch.mean(torch.stack(all_new_logit_diff), dim=0)\n",
    "        var_logit_diff = torch.var(torch.stack(all_new_logit_diff), dim=0)\n",
    "\n",
    "        #fig, ax = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "        sns.heatmap(avg_logit_diff, ax=ax[1][0], vmin=-1, vmax=1, xticklabels=[\"CLS\", \"Edited Obj\", \"Non-edited Obj\"])\n",
    "        ax[1][0].set_title(\"Average logit difference across base images\")\n",
    "        sns.heatmap(var_logit_diff, ax=ax[1][1], vmin=0, vmax=1, xticklabels=[\"CLS\", \"Edited Obj\", \"Non-edited Obj\"])\n",
    "        ax[1][1].set_title(\"Variance in logit difference across base images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c216069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1780...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a8b28d5c5a4ac2963116aa10cb08a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                          | 1/100 [00:08<13:39,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 446...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:09<07:06,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 327...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1e4e2d73934a6890da65d4f1449101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:17<09:35,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2564...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8eadf1e6e9c41c5b8dc8f1e6d5ae2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:25<10:45,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2002...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [00:27<07:39,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2335...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5738050d8674481864d7a29e0f3189e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [00:35<09:17,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 690...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [00:36<06:52,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1542...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [00:38<05:23,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3069...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6faf7389dc48b69dc6bedc60698f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [00:45<07:15,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1599...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129186c6246440ba891984b563911fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [00:53<08:38,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b6f2b701034131bbec79e6eafd58d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [01:01<09:28,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1723...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339aeea9792e4fd69448e804ac355dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [01:09<10:14,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 497...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42151999ffff4015ba088a27e0c57584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [01:18<10:41,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2011...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b945de9e1f374362a497bbbab3238b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [01:26<11:08,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2549...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8652312ad78b49ca907b25b491725c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [01:35<11:23,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1711...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff4d43f58dc47cc8df190a4a8d5fe3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [01:43<11:11,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [01:45<08:40,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2689...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa26f19556f0477abfa83cb2a34a836b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [01:54<09:37,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 208...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0132d7dc8e4928aacff989b4997ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [02:03<10:11,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1985...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97063b0d322640b28dab5de18b5be450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▍                                 | 20/100 [02:11<10:19,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 844...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01efe9060b6427eb849c0b521e5bc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████▊                                 | 21/100 [02:18<10:10,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1441...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a6d1d89673460d9894ec951749ff02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████▏                                | 22/100 [02:26<10:05,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 422...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb130cd6f2e34800bcb5018d08f9682b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▋                                | 23/100 [02:34<09:57,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 817...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b54caed0a164868b3f5f51106c55931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████                                | 24/100 [02:43<10:15,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 609...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663a0f1852884b0fb12b2a5313c37897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████▌                               | 25/100 [02:51<10:09,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1192...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10df24701a74b7f965c6b47f7b5b3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████▉                               | 26/100 [02:59<10:00,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 609...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae1fe1264724d96b7e0b6b7c7a9a053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████▎                              | 27/100 [03:08<09:55,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1702...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|███████████▊                              | 28/100 [03:09<07:25,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 104...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████▏                             | 29/100 [03:10<05:30,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2876...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ac500dbb384395a8cbb6457611cee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████▌                             | 30/100 [03:17<06:21,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1901...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████                             | 31/100 [03:19<04:46,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2350...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████▍                            | 32/100 [03:20<03:38,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1411...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc9d2f1883643b38b0b46f59359ddb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████▊                            | 33/100 [03:29<05:33,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 327...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391c872e804945beb230f0261f5808e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|██████████████▎                           | 34/100 [03:37<06:31,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 877...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4918aba116249b7b583abfa21881d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████▋                           | 35/100 [03:45<07:07,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1506...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b746da35e84430ca465d75362906c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████                           | 36/100 [03:53<07:24,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2727...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d2073b508f48aa98aaf17b0dc71a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███████████████▌                          | 37/100 [04:01<07:36,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 954...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a567557144a7a9a7960df94292ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████▉                          | 38/100 [04:09<07:43,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 461...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47033d891fc54a0ea03cf432966cc79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████▍                         | 39/100 [04:17<07:58,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1732...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4981cc87ad5041e2a2bc5efe5b8ea7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████▊                         | 40/100 [04:26<07:59,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3143...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c3d058df254bc0817891f47b756fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████▏                        | 41/100 [04:34<07:47,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████▋                        | 42/100 [04:35<05:45,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2858...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████                        | 43/100 [04:36<04:23,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 425...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b553bf5132eb45f6974af2595af09a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|██████████████████▍                       | 44/100 [04:45<05:19,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1646...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7125406f2ded4c6399f2f9c56f907acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████▉                       | 45/100 [04:52<05:46,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 797...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4b22e6e9614b21aa141351219eff50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████▎                      | 46/100 [05:00<06:04,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2724...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deab24e38a09439b84b2f3701fede575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████▋                      | 47/100 [05:08<06:16,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52efe52512494d77997483f0803ec13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████▏                     | 48/100 [05:16<06:21,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1536...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5d9e7d4a2e4b1a86e0f8e47c00d260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████▌                     | 49/100 [05:24<06:21,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2668...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082d0772e6cc427ebfb87dc4696c50f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████                     | 50/100 [05:32<06:20,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1105...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████████████████████▍                    | 51/100 [05:33<04:42,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2733...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8e2aafc3e84c5c8be29987eab1ff79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████████████████████▊                    | 52/100 [05:41<05:08,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1441...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ccb353432e402e927fc0e34660fdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████▎                   | 53/100 [05:49<05:22,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 909...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5719a481cde4b7194442f3a1988667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████▋                   | 54/100 [05:57<05:30,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1438...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf9aae46f7b4ea083aeb92182f207bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████                   | 55/100 [06:05<05:29,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 981...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c167bf90fd49a58ae37ccf4e4ccfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████▌                  | 56/100 [06:13<05:31,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312f64f6ee9447df8297f1a6737a5d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|███████████████████████▉                  | 57/100 [06:20<05:24,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1652...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aeca5382454e83b93b4581cefba16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|████████████████████████▎                 | 58/100 [06:28<05:20,  7.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2891...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd39d8cd7444f8496bde938360e6ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████▊                 | 59/100 [06:36<05:17,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2561...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cd1a8b867b40e894169164f573fbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████▏                | 60/100 [06:44<05:13,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2813...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e67f2dc10e460a9ea87f3f5a70f3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|█████████████████████████▌                | 61/100 [06:52<05:11,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 473...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████                | 62/100 [06:54<03:49,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1931...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a553754c4d4742b7f0edadc18ebf1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████▍               | 63/100 [07:02<04:08,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2412...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████▉               | 64/100 [07:04<03:08,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 434...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5908e7fc6c1a4e48a8a9ea3302838023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████▎              | 65/100 [07:12<03:29,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2065...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|███████████████████████████▋              | 66/100 [07:14<02:42,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2834...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6214997b909941e9bc3b7714b110d7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████▏             | 67/100 [07:22<03:09,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2534...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18baef6cb662493eb6d0dac35c50addd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████▌             | 68/100 [07:30<03:28,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2772...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████▉             | 69/100 [07:32<02:39,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2326...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602584f9831d45af893da211674b0a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████▍            | 70/100 [07:40<03:02,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1590...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dd33671aa5460091f53b3c533fdee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████▊            | 71/100 [07:48<03:15,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2763...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd35f1b577a44074b37d126779156bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████▏           | 72/100 [07:57<03:22,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2190...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19a3475d7b549d5ad37905270b74f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|██████████████████████████████▋           | 73/100 [08:05<03:20,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 621...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████████████████████████████           | 74/100 [08:07<02:29,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2124...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4737c77e6f54206a22d00b794c2b5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████▌          | 75/100 [08:15<02:44,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 788...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddde53b2b4942ef866e0f32758839ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████████████████████████████▉          | 76/100 [08:23<02:49,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 562...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74af7404403b423aaf880a3b05e14aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|████████████████████████████████▎         | 77/100 [08:31<02:49,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 348...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████▊         | 78/100 [08:33<02:02,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1783...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████▏        | 79/100 [08:34<01:29,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1509...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|█████████████████████████████████▌        | 80/100 [08:36<01:11,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 520...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████        | 81/100 [08:38<00:56,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3081...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de67bb038c545bfb0a6d91069784889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████▍       | 82/100 [08:45<01:19,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3072...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d27e8873614f448dd0af312d664d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|██████████████████████████████████▊       | 83/100 [08:53<01:33,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1108...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c025372564d4b788b28f7b1745d9527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|███████████████████████████████████▎      | 84/100 [09:01<01:39,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 704...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b347102b46e3452c84d01d7ab1b402f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████▋      | 85/100 [09:10<01:43,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 384...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████      | 86/100 [09:11<01:14,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2813...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8d2714706446f6864b96f5e57bfa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████▌     | 87/100 [09:19<01:18,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2210...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85032c31a3eb4e9ea0764749587d8cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████▉     | 88/100 [09:27<01:20,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3096...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86b13b0e89f41b5b0b6bb6db2f9a182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████▍    | 89/100 [09:35<01:17,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f81115195b347419f3bbfe9a2f98a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████▊    | 90/100 [09:43<01:13,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2297...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a73e72b96e44bb687af758e558998b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████▏   | 91/100 [09:51<01:07,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2383...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b744b3888f034c42aaff4c763095291f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████▋   | 92/100 [09:59<01:00,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b892bba090248b9ae09b8c9b87f3ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████   | 93/100 [10:07<00:53,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2932...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d18c2a6ba346689f059cda9da654e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|███████████████████████████████████████▍  | 94/100 [10:15<00:46,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1661...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230acaabef9e4a1b947a1f8aae7cf09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|███████████████████████████████████████▉  | 95/100 [10:25<00:42,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 167...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb0beba2e6e462aa7c88faa95a636ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|████████████████████████████████████████▎ | 96/100 [10:33<00:33,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2552...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|████████████████████████████████████████▋ | 97/100 [10:34<00:18,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 850...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5285b03a61d84dd2a7353fee726f598f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████████████████████████████████████▏| 98/100 [10:42<00:13,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 752...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|█████████████████████████████████████████▌| 99/100 [10:44<00:05,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1842...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8928f37eb53a45069e778473ef3cc8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [10:52<00:00,  6.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Scratch ViT-B/32 subspace score: 0.9061989752869857 (variance=1.4062647170163955)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                             | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2796...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a29ca3c662e4d45928173f3b724ef2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▏                                                                                                                   | 1/100 [00:07<12:44,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 856...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1d45d36f54a21a22d870cce46ea47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▎                                                                                                                  | 2/100 [00:16<13:10,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 99...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c188dbb82184cd98614b5e61f601b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|███▌                                                                                                                 | 3/100 [00:24<12:59,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2920...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae82e50a75844aba71317506d3eaf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|████▋                                                                                                                | 4/100 [00:32<12:51,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 185...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d95138dd1354a5ea89afc2f99789bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|█████▊                                                                                                               | 5/100 [00:40<12:54,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2279...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███████                                                                                                              | 6/100 [00:42<09:21,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2608...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f82d20f3742466bbed561ca30f90b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|████████▏                                                                                                            | 7/100 [00:50<10:17,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3170...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████████▎                                                                                                           | 8/100 [00:51<07:36,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 476...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d89b12fea39465fbfdaf83663ea6120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|██████████▌                                                                                                          | 9/100 [00:59<08:57,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2326...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca9542c03164e9d924aef06806f40b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████▌                                                                                                        | 10/100 [01:07<09:45,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2724...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cc3eb4722d4f40897cec4e5349d76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████████▊                                                                                                       | 11/100 [01:15<10:18,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1307...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7019e8b1fcd42209572860a2ffc0826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████████▉                                                                                                      | 12/100 [01:23<10:35,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3122...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cf5216425f44f887a8c54847f59094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████████                                                                                                     | 13/100 [01:30<10:41,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1492...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████████▏                                                                                                   | 14/100 [01:32<08:02,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 863...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a408f53ca8104a12b4ad0fabff62cf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█████████████████▍                                                                                                  | 15/100 [01:40<08:51,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1939...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████▌                                                                                                 | 16/100 [01:41<06:51,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2499...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee0f1bf621a43edac06b97d39b12823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████████▋                                                                                                | 17/100 [01:50<08:28,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1783...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d09a52ff54477da315a3c84cb14a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████████████████▉                                                                                               | 18/100 [02:00<09:45,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 666...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83334a7d72b446a19b76e97e526bc494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████████████                                                                                              | 19/100 [02:09<10:25,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▏                                                                                            | 20/100 [02:11<07:48,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3185...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6bae964dea4eb49f45a6157da91467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████████████▎                                                                                           | 21/100 [02:19<08:37,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2834...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8664d933b14d2bae66528c8b2a81c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████████████▌                                                                                          | 22/100 [02:26<08:59,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2017...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489214155f4a42dcb8595f40feb8b78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████████████▋                                                                                         | 23/100 [02:34<09:14,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1599...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039541dbd4ea466499ec9107097ab32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████████████▊                                                                                        | 24/100 [02:43<09:34,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c13f2c014e48efb65f5649ad0dc82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████████████                                                                                       | 25/100 [02:51<09:46,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 202...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac463d3d2aa7409e99a735175ffe3e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████████████████████████▏                                                                                     | 26/100 [03:00<09:50,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1942...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████████████████████████▎                                                                                    | 27/100 [03:01<07:15,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1385...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbd48bdbdd248bf8cff90725f5517fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████████████████████████▍                                                                                   | 28/100 [03:09<07:59,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2531...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|█████████████████████████████████▋                                                                                  | 29/100 [03:10<05:54,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 710...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f718eecd99754c2f9995270e728f7a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████▊                                                                                 | 30/100 [03:18<06:57,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 464...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9b46d5ee3e4b29a30921a89b90c2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████▉                                                                                | 31/100 [03:26<07:30,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1186...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41057067f1244a939b1225d40fefd0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████████████████                                                                               | 32/100 [03:34<07:49,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1291...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████████████▎                                                                             | 33/100 [03:36<05:59,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 685...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d610807f8ecc45dbaeaeb6f04693b115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████████████████▍                                                                            | 34/100 [03:44<06:51,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2683...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2975f081c39a422eb76bf296d56cba70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████████████████▌                                                                           | 35/100 [03:52<07:15,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1010...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfc530f3e4040378c2cd97d9b02f098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████████████████▊                                                                          | 36/100 [04:00<07:40,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1438...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc889776b5b4eafa0a2241509c018c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████████████████▉                                                                         | 37/100 [04:08<07:50,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5b5329f75046e7a554a691e49e7232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████████████████████                                                                        | 38/100 [04:16<07:54,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 866...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645a8e27f6814d4c91321e876108bc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████████████████████▏                                                                      | 39/100 [04:24<07:53,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2234...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbf14327d8e4dc0b9e6f02781380bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████████████████████▍                                                                     | 40/100 [04:32<07:49,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1474...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████████████████████▌                                                                    | 41/100 [04:34<05:50,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 488...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd197d25f84c4753bfe14a4b93f2cfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████████████████████▋                                                                   | 42/100 [04:42<06:16,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3096...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8987a76f803a4a6d8b541b078e2c2d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████████████████████████████▉                                                                  | 43/100 [04:50<06:35,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3033...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183b547dd6154416bb12008996fe67dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████████████████████                                                                 | 44/100 [04:57<06:43,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1111...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████████████████████▏                                                               | 45/100 [04:59<04:58,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1417...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac784b78146b43899759a0361bc8efa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████████████████████▎                                                              | 46/100 [05:07<05:35,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1959...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████████████████████████████████████████▌                                                             | 47/100 [05:08<04:10,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1240...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab0949e6db04afdb825a52e17a28e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████████████████████▋                                                            | 48/100 [05:16<05:03,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████████████████████████████████████▊                                                           | 49/100 [05:18<03:53,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1900...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████████████████████                                                          | 50/100 [05:19<02:57,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 906...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████████████████████████▏                                                        | 51/100 [05:21<02:24,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 387...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97af8ea07ca4ce981decc520b0a482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████████████████████████▎                                                       | 52/100 [05:29<03:36,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1233...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be31d167c66a47638704ef46c390afc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████████████████████████▍                                                      | 53/100 [05:37<04:18,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2908...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484f60e19b25453aa197ab8fb6f2d807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████████████████████████▋                                                     | 54/100 [05:45<04:54,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 889...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932edb4637f64d758993c21eaee83607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████████████████████████████████████▊                                                    | 55/100 [05:53<05:10,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fd0460b9644db284d4767c89ae3dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████████████████████████████▉                                                   | 56/100 [06:02<05:22,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1566...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37bb5dd6f2443a3852d73086db8ecf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████████████████████████                                                  | 57/100 [06:10<05:26,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1771...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66b5d30f95f47f587e37f6630b2fcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████████████████████████▎                                                | 58/100 [06:18<05:23,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2053...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c202b532906946e0a6d94077b1bc41bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████████████████████████▍                                               | 59/100 [06:26<05:15,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 921...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15af1013eac341d9b5cf7af011ae75a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████████████████████████▌                                              | 60/100 [06:33<05:10,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1960...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████████████████████████████████████████████████████████████████████▊                                             | 61/100 [06:35<03:46,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1900...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████▉                                            | 62/100 [06:36<02:48,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2160...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b714ea1e0a0543fba6de1f7c0bb5a90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████████████████████████████████                                           | 63/100 [06:44<03:23,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 280...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345bd6adb08e456daeef99c0c605b75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████████████████████████████████████████████████▏                                         | 64/100 [06:52<03:42,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 431...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d2fc71e14e4e85bbe2182a63f7f16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████████████████████████████████▍                                        | 65/100 [07:00<03:54,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2712...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0bcf8e578744ce9637912722287d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████████████████████████████▌                                       | 66/100 [07:07<03:59,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 865...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3733658572642aea10fd50952f05b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████████████████████████████████████████████████████▋                                      | 67/100 [07:16<04:04,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1982...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce70b901ce9d43958acb6b6c1e2e71e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████████████████████████████▉                                     | 68/100 [07:24<04:02,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 658...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f9b549c22048419dee1592d40c41c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████████████████████████████                                    | 69/100 [07:32<03:59,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 987...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 70/100 [07:33<02:54,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376b360d745a43e4a41ad6c6ddc4e7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████████████████████████████████▎                                 | 71/100 [07:41<03:07,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 731...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e37165e87514a2887eed6808842df62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████████████████████████████████▌                                | 72/100 [07:49<03:12,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2894...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956052a4a40f49c498295a39badffdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████████████████████████████▋                               | 73/100 [07:57<03:13,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1049...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf12d1530284b5a832c412620f9290d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████▊                              | 74/100 [08:05<03:11,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2115...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a49e88faa264e68afbfb30c5482e16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████████████████████████████                             | 75/100 [08:13<03:11,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 179...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb887899b1d4f869e68f2171dce864e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████▏                           | 76/100 [08:21<03:05,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1397...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16e832292fb45868e00032ea99b7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████▎                          | 77/100 [08:29<03:00,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 88...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73682d116b894cfcab88bc437beb1b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████▍                         | 78/100 [08:37<02:54,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2249...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7d0db4bbbc4ef69f03640a5bd1ce3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████▋                        | 79/100 [08:45<02:46,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 89...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 80/100 [08:48<02:06,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 610...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c1bf01dc94496d8d2def081ae710b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████▉                      | 81/100 [08:55<02:08,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2902...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98bf1b3e18345d8902f28fdf43cd1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████                     | 82/100 [09:03<02:08,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 283...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c627dda44434d59ad67cb423a5ffb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 83/100 [09:12<02:09,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2020...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7552594de1bc4a3cbf40962b83a8cbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                  | 84/100 [09:22<02:10,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1917...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 85/100 [09:23<01:32,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1967...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535e3c670e424f6197de314ecdd51d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                | 86/100 [09:32<01:37,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 87/100 [09:34<01:10,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3170...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████              | 88/100 [09:36<00:51,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1609...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 89/100 [09:38<00:39,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1827...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2947e0f2603041b0ba8b33e81f8ba45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 90/100 [09:47<00:53,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 829...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b59575227524fc28ed4724aeb4de721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 91/100 [09:57<01:00,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1975...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 92/100 [09:58<00:41,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1135...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 93/100 [10:00<00:28,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3060...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baaeb8a3f6524a7db3336bbd6f6ffd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 94/100 [10:09<00:33,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1630...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 95/100 [10:11<00:21,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3057...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23484b35fa2a4bbea45146fad2fca0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 96/100 [10:21<00:24,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 612...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 97/100 [10:22<00:14,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1441...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26036354b0fb4c2bbb96931133ac162b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 98/100 [10:32<00:12,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 101...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1d0599aacb40c1a05861b462d61093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 99/100 [10:41<00:07,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 857...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48b98fe5aaf43399a579cdbc746e657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [10:50<00:00,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Scratch ViT-B/32 subspace score: 0.682238458781629 (variance=0.5405487430154986)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                             | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 752...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▏                                                                                                                   | 1/100 [00:01<01:59,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2724...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d0f5b9291b433da0a2350417ebc228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▎                                                                                                                  | 2/100 [00:11<10:39,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1613...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|███▌                                                                                                                 | 3/100 [00:12<06:39,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2285...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3baf7c371e624f9b98e23d1aaa6991a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|████▋                                                                                                                | 4/100 [00:22<10:07,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1878...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262f811f9cdb450f93b8576acb047627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|█████▊                                                                                                               | 5/100 [00:31<11:51,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2032...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dddc8658294903b55f6b001152544f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███████                                                                                                              | 6/100 [00:41<13:03,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 725...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34434408b0c4fc389830f0eb79ed4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|████████▏                                                                                                            | 7/100 [00:51<13:26,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 853...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1f3d6e59bb4a4ba326dcd07754c73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████████▎                                                                                                           | 8/100 [01:01<14:01,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2341...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ec7c504ff34afe89963d0c10073c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|██████████▌                                                                                                          | 9/100 [01:10<13:39,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2608...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f97937fb0640948017171e7271bcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████▌                                                                                                        | 10/100 [01:18<13:22,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1750...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af44e6f869b042c0b807e045993a0e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████████▊                                                                                                       | 11/100 [01:27<12:55,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 820...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b01f50c03545aa9ea7417ac82fc57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████████▉                                                                                                      | 12/100 [01:35<12:26,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 54...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c4b067c74f4df7b65e829c17ab093c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████████                                                                                                     | 13/100 [01:42<12:01,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2151...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63ba42866644f1c87a554a60c8d0110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████████▏                                                                                                   | 14/100 [01:50<11:43,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2196...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1091ce8c95574d78be8678c1cf916be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█████████████████▍                                                                                                  | 15/100 [01:59<11:48,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2127...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de4c2efc884225bc87dccb300b9428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████▌                                                                                                 | 16/100 [02:07<11:31,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2077...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0085fd03d7a3404ebad05d88c5261068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████████▋                                                                                                | 17/100 [02:15<11:14,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1738...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████████████████▉                                                                                               | 18/100 [02:16<08:19,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3045...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c8a5943c3e4e5a896bd3e4b25aa843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████████████                                                                                              | 19/100 [02:24<09:03,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1349...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▏                                                                                            | 20/100 [02:26<06:50,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1144...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████████████▎                                                                                           | 21/100 [02:27<05:21,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2433...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████████████▌                                                                                          | 22/100 [02:29<04:08,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3167...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670f0f7a94474514bc3a38ab4aae1d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████████████▋                                                                                         | 23/100 [02:38<06:37,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 173...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbb34974cad4d088d4237b2c2bfb47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████████████▊                                                                                        | 24/100 [02:48<08:24,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1548...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52040d588964df0b12ac56dc7198cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████████████                                                                                       | 25/100 [02:58<09:33,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1001...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24498ff81a8b4542b07becb36bdf159d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████████████████████████▏                                                                                     | 26/100 [03:07<09:45,  7.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1358...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee03ff66317c4411b21f2b8f0b249582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████████████████████████▎                                                                                    | 27/100 [03:15<09:43,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1328...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ea8ba3cb474a34a9806dd4e498ff80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████████████████████████▍                                                                                   | 28/100 [03:24<09:42,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3128...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530f7a4e6a794f76892d3cd54dab6ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|█████████████████████████████████▋                                                                                  | 29/100 [03:32<09:31,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370570b39fac4452b585dcefe8a45e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████▊                                                                                 | 30/100 [03:40<09:28,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 871...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74961358ee04dff9e0ce8d8511d8695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████▉                                                                                | 31/100 [03:48<09:15,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 99...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce47a22f6cf346c5b1a107d19edc44ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████████████████                                                                               | 32/100 [03:56<09:16,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2481...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a533b303f54517abc8cfcb627a4f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████████████▎                                                                             | 33/100 [04:04<09:01,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 104...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40f9bfc16b44348bb2a55186a258c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████████████████▍                                                                            | 34/100 [04:12<08:52,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1474...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bd5d63a181408bb9207db5698b44e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████████████████▌                                                                           | 35/100 [04:20<08:44,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1394...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e6d25062d84a23a6a179113951e739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████████████████▊                                                                          | 36/100 [04:28<08:28,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1278...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e87301840ec420da485c97a158f8e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████████████████▉                                                                         | 37/100 [04:36<08:18,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1367...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb1d92fdb964016aff4228bb5d5e6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████████████████████                                                                        | 38/100 [04:43<08:09,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1946...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████████████████████▏                                                                      | 39/100 [04:45<06:05,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2810...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████████████████████▍                                                                     | 40/100 [04:47<04:39,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 188...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b1e08c84ce40a1aaad7a200cc64e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████████████████████▌                                                                    | 41/100 [04:55<05:33,  5.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2787...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6264fd45a0941fab451db4d187cd3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████████████████████▋                                                                   | 42/100 [05:02<06:05,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 298...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████████████████████████████▉                                                                  | 43/100 [05:04<04:36,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0906d0973e4f399b7ed05f1a65aef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████████████████████                                                                 | 44/100 [05:12<05:24,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2671...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d12ca4b2af45698366d79c85f221f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████████████████████▏                                                               | 45/100 [05:20<06:01,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1013...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f316378b958844549bc4d0a656c48088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████████████████████▎                                                              | 46/100 [05:28<06:16,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2005...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49b579748564210802702e98715748c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████████████████████████████████████████▌                                                             | 47/100 [05:36<06:25,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2210...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa4638b0ef34bd6b191abb593f44555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████████████████████▋                                                            | 48/100 [05:44<06:23,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3009...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1682532fad441e9a487a55ac1c803f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████████████████████████████████████▊                                                           | 49/100 [05:52<06:27,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 678...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af4cc6ee086478f932c9984dfbc4004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████████████████████                                                          | 50/100 [06:00<06:24,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2724...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a689b46a5c86446ca8bdfde3c402d5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████████████████████████▏                                                        | 51/100 [06:08<06:26,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2959...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████████████████████████▎                                                       | 52/100 [06:09<04:45,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1913...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93333105c645406b9b997986b6bd1994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████████████████████████▍                                                      | 53/100 [06:17<05:00,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2638...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1431c15a4cae4888a310ec31b6862323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████████████████████████▋                                                     | 54/100 [06:25<05:16,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1132...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07961ca61334bb9a29c59502f957f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████████████████████████████████████▊                                                    | 55/100 [06:33<05:21,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1521...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7f49dd7f8b42eba33116e4a806265b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████████████████████████████▉                                                   | 56/100 [06:41<05:24,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1168...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93641e00139247d19b803db7b6930a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████████████████████████                                                  | 57/100 [06:50<05:45,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1708...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318db96767b742c28c3d3a48d46bbc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████████████████████████▎                                                | 58/100 [06:59<05:44,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 131...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d986b597a17247a4bb7a050c514dcaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████████████████████████▍                                               | 59/100 [07:08<05:49,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3090...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebe02d3e1ad4da2bb3e8e0026ba6cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████████████████████████▌                                              | 60/100 [07:16<05:31,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2911...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0780eb922d409dbb6a8b8b0ca1fa63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████████████████████████████████████████████████████████████████████▊                                             | 61/100 [07:24<05:25,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1640...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ccdca9d7014f57b48f8786c4407060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████▉                                            | 62/100 [07:33<05:16,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1860...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3f9bf686374710b9ec765a76dad816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████████████████████████████████                                           | 63/100 [07:41<05:06,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 999...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2521415ee2458bbdb5a8599bd318c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████████████████████████████████████████████████▏                                         | 64/100 [07:49<04:58,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2326...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████████████████████████████████▍                                        | 65/100 [07:50<03:37,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1708...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fef3cf094534f7d8d186fdefc72218a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████████████████████████████▌                                       | 66/100 [07:58<03:48,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3158...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d448760a5f1e42e9855d4b77db107428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████████████████████████████████████████████████████▋                                      | 67/100 [08:06<03:52,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 333...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5524f05ccb84dcc917b1fa3b34cffeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████████████████████████████▉                                     | 68/100 [08:14<03:55,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1616...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2634dbc14a946cc838fba2eae25ff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████████████████████████████                                    | 69/100 [08:22<03:51,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2965...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb293367be741678a4f6e1358265e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 70/100 [08:30<03:48,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 844...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eae3b7a86b845ab952c2032a15243ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████████████████████████████████▎                                 | 71/100 [08:38<03:42,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2929...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e179c9cbc01041c086f37d37a5618f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████████████████████████████████▌                                | 72/100 [08:46<03:40,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1590...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3171f2dc9814831a0648f5c12cc0bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████████████████████████████▋                               | 73/100 [08:55<03:40,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1777...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f04a3b8969641d8b13179b2af7ab834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████▊                              | 74/100 [09:03<03:29,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1016...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e23fdc183c147f2bd854f574dd0e27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████████████████████████████                             | 75/100 [09:11<03:21,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2083...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1676e1a999974851b5fa4cdf393ad7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████▏                           | 76/100 [09:18<03:11,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2561...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f86963f113147f9aa8a17fe48ddca94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████▎                          | 77/100 [09:26<03:02,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 553...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d6ebb4302040b984f9dff25758116f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████▍                         | 78/100 [09:34<02:53,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2121...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60134ef5de6140f0aab8cc89ccdd4801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████▋                        | 79/100 [09:42<02:44,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 883...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d398d55a0b3a46af9c232d60dae44e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 80/100 [09:51<02:45,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2353...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe2edf3e5854692a7f30fbd6b728855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████▉                      | 81/100 [10:01<02:47,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237cc1658ead47e5888bad90934807af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████                     | 82/100 [10:11<02:43,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1572...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c188e934868c4f6d8e5dd7e708b1b175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 83/100 [10:18<02:26,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 434...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea0b651a7b74c0098d59b3171d7e2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                  | 84/100 [10:27<02:15,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1284...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42df83e57ccc43dbad4a244f702ddc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 85/100 [10:35<02:07,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a86621a489f4ac39b8a97c174f01a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                | 86/100 [10:44<02:01,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 48...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8078ee9f7cd14bd789f57f02564a33fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 87/100 [10:53<01:53,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2243...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4da2e038ebf4eca9e09dea4a4438895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████              | 88/100 [11:02<01:46,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1028...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4863a66162b4d32b7a8a3c5b99ebf40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 89/100 [11:12<01:41,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2047...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de43b55801b343e59040c2dba310cec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 90/100 [11:22<01:34,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2638...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c47f38fe784529813f9e969b8bb13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 91/100 [11:32<01:26,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2282...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec36754457cf44a39e51ecc14fbf5a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 92/100 [11:42<01:16,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1120...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2d2c5059e3476d93c47c1a9bd81c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 93/100 [11:50<01:05,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 446...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d52ec256104740aa34e97563664f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 94/100 [12:00<00:55,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1717...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc41f4157de4d3c90f5ee0643d4e8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 95/100 [12:09<00:46,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3099...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf2e9c2672444709ec41d423946bc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 96/100 [12:19<00:37,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1462...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eda6a9f1d7a401a9cf6e99177954295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 97/100 [12:28<00:28,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 194...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcbebc62c074077af5c3b02492d204a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 98/100 [12:36<00:17,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 149...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fcfab180e74fbbb1088bc41f99293d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 99/100 [12:45<00:08,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 942...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9814119db54cb390f2742cbc05e58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [12:55<00:00,  7.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-Pretrained ViT-B/32 subspace score: 0.7129830771617676 (variance=0.37662975931465525)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                             | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1988...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57e7796f2d640bfbc02c0e23662cd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▏                                                                                                                   | 1/100 [00:10<16:53, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2181...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea5271310fe4cd982388ee978b4ad03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▎                                                                                                                  | 2/100 [00:19<15:56,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137856d2cdbd425a92f5641238611000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|███▌                                                                                                                 | 3/100 [00:27<14:23,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69741bb620fe4127bdc2659aea5468f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|████▋                                                                                                                | 4/100 [00:36<14:17,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1273...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418cfc15cca541a38ff24f48e187dcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|█████▊                                                                                                               | 5/100 [00:45<14:01,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2068...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f7964d7398468d9f79467a2a5f32dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███████                                                                                                              | 6/100 [00:54<14:01,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 667...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b5efcc0d9741e7aabb416fe556857b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|████████▏                                                                                                            | 7/100 [01:03<14:04,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2258...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████████▎                                                                                                           | 8/100 [01:05<10:18,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1747...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8366f4f240c74d74bdce65aad9360d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|██████████▌                                                                                                          | 9/100 [01:15<11:38,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1180...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330078e10a924d72b7a742bb0814623c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████▌                                                                                                        | 10/100 [01:24<12:13,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2689...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72c327ebdf542a4b48584da7a93629c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████████▊                                                                                                       | 11/100 [01:32<12:07,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2522...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0866a2eaedf64b19bd215eab0daea9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████████▉                                                                                                      | 12/100 [01:40<12:04,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2184...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████████                                                                                                     | 13/100 [01:42<09:10,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2196...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705ab20dc46c4f24a6288cf28b70f4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████████▏                                                                                                   | 14/100 [01:51<10:14,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2698...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38ea11892fd4bea93d0d0462a2e9783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█████████████████▍                                                                                                  | 15/100 [02:01<11:14,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1768...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c62788f57b94315af070425854b0d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████▌                                                                                                 | 16/100 [02:11<11:56,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1270...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2847aed0b867431e9946ba62bb9ea8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████████▋                                                                                                | 17/100 [02:20<12:02,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 580...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3195b54e584e4f8c01343e8b61b2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████████████████▉                                                                                               | 18/100 [02:30<12:17,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2968...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2717003bbd184609b28a89d1b8876b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████████████                                                                                              | 19/100 [02:40<12:31,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 247...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379185b149a14c58add8c26f25cc914d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▏                                                                                            | 20/100 [02:50<12:33,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 681...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0ce510cf994c818f5784d4c4612b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████████████▎                                                                                           | 21/100 [02:59<12:17,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1649...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2702fc43dd4f5b8f646831834774ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████████████▌                                                                                          | 22/100 [03:07<11:46,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1907...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817c12d3ac1746bb8920e32b610565ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████████████▋                                                                                         | 23/100 [03:15<11:20,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1052...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a9982f95d34f298a9e0674b615a1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████████████▊                                                                                        | 24/100 [03:24<11:07,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1688...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1971dde714604e99978c6429e9e8d848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████████████                                                                                       | 25/100 [03:34<11:12,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 97...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbd96c4b59842a690cd17cb8151237f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████████████████████████▏                                                                                     | 26/100 [03:43<11:11,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2724...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e720dfe5af0a4e0f8f2564602a64044f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████████████████████████▎                                                                                    | 27/100 [03:51<10:50,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1750...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8c7a0a16cf48939dec89ff62d75a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████████████████████████▍                                                                                   | 28/100 [04:01<10:45,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1930...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|█████████████████████████████████▋                                                                                  | 29/100 [04:03<08:16,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1672...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████▊                                                                                 | 30/100 [04:04<06:05,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 363...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54faaca6f71240309b15ea978c0e08f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████▉                                                                                | 31/100 [04:13<07:28,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1890...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5313b52dff064645a2c7b8020ae2ca03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████████████████                                                                               | 32/100 [04:23<08:24,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2210...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a04416dd84e4f1aaf401a7a0e1e12b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████████████▎                                                                             | 33/100 [04:32<08:48,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2876...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5196bbbf6b344f50a1c27fb9a927200c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████████████████▍                                                                            | 34/100 [04:41<09:06,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1261...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0702e06e8d434bd9bda82653ed68c3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████████████████▌                                                                           | 35/100 [04:51<09:19,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3113...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e05f519a0dc4cbca4c016be93738a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████████████████▊                                                                          | 36/100 [05:00<09:34,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1450...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43a2b219278400bbf2fd414fccfefef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████████████████▉                                                                         | 37/100 [05:11<09:47,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1281...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4668a6f120944c229dc3774258e406c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████████████████████                                                                        | 38/100 [05:21<09:53,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1236...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322b9146b45a4563b2b35b20a1372792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████████████████████▏                                                                      | 39/100 [05:31<09:49,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d45e4d94976441794ef73be9c064dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████████████████████▍                                                                     | 40/100 [05:39<09:19,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 306...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f32139bc7a84adeab6a4b82a24c514e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████████████████████▌                                                                    | 41/100 [05:48<09:07,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2291...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde249cadce64f88ab8d4f1182ccd820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████████████████████▋                                                                   | 42/100 [05:57<08:52,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 836...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754d8f92b8c647919aa1c5920a248285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████████████████████████████▉                                                                  | 43/100 [06:07<08:53,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2888...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7da903d661b4b6b81720bb90ab55d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████████████████████                                                                 | 44/100 [06:16<08:43,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2308...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce67c2f695044777a45d1017c43ae607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████████████████████▏                                                               | 45/100 [06:25<08:29,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 610...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9970590b323849faa63a14e4aa7acf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████████████████████▎                                                              | 46/100 [06:35<08:19,  9.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1408...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5604b6751f28415cbd38bf4adf3b5ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████████████████████████████████████████▌                                                             | 47/100 [06:44<08:11,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 327...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beb678947de4d98a7fd819ff6fde748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████████████████████▋                                                            | 48/100 [06:53<08:06,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2335...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c55640e6e94127ac5c5edb079c4f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████████████████████████████████████▊                                                           | 49/100 [07:03<07:54,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2700...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64822b74ddf414791881336ef26ba22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████████████████████                                                          | 50/100 [07:12<07:45,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 895...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████████████████████████▏                                                        | 51/100 [07:14<05:45,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 933...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2e74e596e34cb0bad539e91a200b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████████████████████████▎                                                       | 52/100 [07:22<05:53,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2715...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d4cff086804e14967a5da0838d0868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████████████████████████▍                                                      | 53/100 [07:31<06:04,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2098...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d253eb5d704b6ead1528a1ce27489d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████████████████████████▋                                                     | 54/100 [07:39<06:05,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 939...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9699d2f33d2475da9f5c3313be3c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████████████████████████████████████▊                                                    | 55/100 [07:47<06:03,  8.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2911...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465ffc5d1be242cb93fac9a4b30a5e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████████████████████████████▉                                                   | 56/100 [07:57<06:14,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1070...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be200de3ff974147a771071dfe0fad6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████████████████████████                                                  | 57/100 [08:06<06:18,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 494...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce150c0409e40c1955323351dd33541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████████████████████████▎                                                | 58/100 [08:16<06:19,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2166...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bcb83cb93445de9ef46dcf0f4d47df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████████████████████████▍                                               | 59/100 [08:25<06:13,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1474...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6568fe2eb4ea452194e10711aee87ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████████████████████████▌                                              | 60/100 [08:34<06:01,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2068...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985c99bc338a49158668885cfa5170bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████████████████████████████████████████████████████████████████████▊                                             | 61/100 [08:43<05:54,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1468...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58f72408d594c9baabffba70f43c120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████▉                                            | 62/100 [08:53<05:56,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 627...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7514ac4ed8464ca03e54cc479a045a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████████████████████████████████                                           | 63/100 [09:02<05:35,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1512...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8f50e4444a4a56b6e7e3873282c0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████████████████████████████████████████████████▏                                         | 64/100 [09:10<05:16,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3146...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b5867b71e94d61821598c5db49d63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████████████████████████████████▍                                        | 65/100 [09:18<05:05,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2790...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4ad34a62cc4c3998f7dcd2ab83d537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████████████████████████████▌                                       | 66/100 [09:27<04:58,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8784cf68ac2248c9a08db8fb1ddf9342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████████████████████████████████████████████████████▋                                      | 67/100 [09:36<04:52,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 639...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adfaad799554775996b0df6b2272a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████████████████████████████▉                                     | 68/100 [09:46<04:48,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 978...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506dd655455149a2a41e75a4720fd92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████████████████████████████                                    | 69/100 [09:55<04:37,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1435...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef1fa2919584f37a346dddcb987d437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 70/100 [10:04<04:32,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2311...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75996f7c20fb485bb202af041a6c76e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████████████████████████████████▎                                 | 71/100 [10:13<04:26,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2846...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384b8bca08b448758aa013a8ae784d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████████████████████████████████▌                                | 72/100 [10:22<04:16,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 86...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████████████████████████████▋                               | 73/100 [10:24<03:05,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1655...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeb8d79d5304b06a0cba7f0cea279d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████▊                              | 74/100 [10:33<03:13,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1521...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d14a66ee7ef4ac3a1836fb54243d5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████████████████████████████                             | 75/100 [10:41<03:14,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1219...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645a583d48324d4692c42830af47083e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████▏                           | 76/100 [10:51<03:19,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 532...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd80d353f5494875971d9c0f1e46831a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████▎                          | 77/100 [11:00<03:16,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 853...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446348d7cf0b48f3bcedb0032f8e3492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████▍                         | 78/100 [11:09<03:11,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2828...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2597d20a843445f48d05cf105836cc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████▋                        | 79/100 [11:17<03:00,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 390...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362dc675b46f413194dfba77e13194ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 80/100 [11:27<02:58,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1974...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a521b7c5584d8e8732d9b8a28baa5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████▉                      | 81/100 [11:36<02:50,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1905...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f74334276d4b7f90cfa79800cbc850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████                     | 82/100 [11:46<02:45,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 399...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54091ed056774ecd8e74b80d859317c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 83/100 [11:54<02:31,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2902...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd3e2772cc44d1a9388ec3331e2f9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                  | 84/100 [12:04<02:27,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2199...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293961494c4d409dae4aa2f315bdc875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 85/100 [12:13<02:17,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 339...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9214027043c54dbc88b547e9ef3fcedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                | 86/100 [12:23<02:09,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 94...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c5b41b617e4ef1afab25e539cb0f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 87/100 [12:32<02:00,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1147...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074a6aa126c841e7bbccb5fe24246098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████              | 88/100 [12:41<01:50,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b433b5be3642f3a49b16e779c443f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 89/100 [12:50<01:40,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 193...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 90/100 [12:51<01:08,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 482...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd285ab0e5c4dc7a237a0c220698ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 91/100 [13:00<01:06,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 303...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6a8c1e0a384d7d805096e2c3abbec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 92/100 [13:10<01:05,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2980...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7755cd2110401087befd7d590fc728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 93/100 [13:19<00:58,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 592...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027b85f5e0e543d4a575fcb027c9deb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 94/100 [13:28<00:51,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2614...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2a9b00c9874641a0f9faec8429953e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 95/100 [13:36<00:42,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2504...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b4bbb3ed9f4d779bd04c6ac1c9341e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 96/100 [13:46<00:35,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1462...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bf2a6add334259bb7606990e240b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 97/100 [13:55<00:26,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 188...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec8c30dcee244e2b20e823fbbb931ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 98/100 [14:05<00:18,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1800...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ca4b79ff154ed094257e6306659c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 99/100 [14:14<00:09,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1447...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e598e8419e43f38d61ed040dcd26e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [14:23<00:00,  8.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet-Pretrained ViT-B/32 subspace score: 0.437870091648512 (variance=0.6204976766257226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                             | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1275...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6186b3ea18499a85c0d583963a2e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▏                                                                                                                   | 1/100 [00:10<16:52, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1899...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302f634ed7cc4b5d88b42b5ad5be4277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▎                                                                                                                  | 2/100 [00:19<15:41,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 918...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce71b43ce0244f4c9f961d8b68f78788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|███▌                                                                                                                 | 3/100 [00:29<15:46,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1031...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9453f86ab34a858ca5bcc890f169da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|████▋                                                                                                                | 4/100 [00:40<16:20, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1560...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589f8bc215b34fbf83f85fd01cf23611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|█████▊                                                                                                               | 5/100 [00:51<16:38, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2112...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ec9e20bfa748128e72a7829dff90d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███████                                                                                                              | 6/100 [01:01<16:13, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1572...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94335d39139749aba365fc3e69a31802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████████▏                                                                                                            | 7/100 [01:12<16:23, 10.58s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 761...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed444f96915a4c15a6031fed8b441a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████████▎                                                                                                           | 8/100 [01:23<16:21, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 110...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b69088bd184363a00567218bc2ebc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|██████████▌                                                                                                          | 9/100 [01:34<16:23, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1352...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799c0eb7b60945699385d46bf4272454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████▌                                                                                                        | 10/100 [01:44<15:59, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1673...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d4679f7f77483f81f359e4e9dc693c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████████▊                                                                                                       | 11/100 [01:56<16:23, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 565...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb574c37ddd4db6a8b2fce90b2a5d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████████▉                                                                                                      | 12/100 [02:06<15:42, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2457...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea4d0e074404304acf66995992cbbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████████                                                                                                     | 13/100 [02:16<15:13, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1747...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cca1108e944c7987da37c4dba625b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████████████▏                                                                                                   | 14/100 [02:27<15:02, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2222...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827936bbdb224fcc85bb149e20d7469c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████▍                                                                                                  | 15/100 [02:35<14:11, 10.02s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1812...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c377cd516745db94f7867b424c2ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████▌                                                                                                 | 16/100 [02:45<13:57,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 54...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8888a1efa540c8a7007f8feea520de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████████▋                                                                                                | 17/100 [02:56<14:04, 10.17s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3143...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b709383dbfa24b059ac38b573707ffcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████████████████▉                                                                                               | 18/100 [03:05<13:16,  9.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 782...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7bd71491f04f41b1f528f24324105a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████████████                                                                                              | 19/100 [03:15<13:16,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2430...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379c7e9a7bbb44a4ada43073d99fbb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████████████████▏                                                                                            | 20/100 [03:24<13:02,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3018...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0a949a8bd14524974d84ba59384362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████████████▎                                                                                           | 21/100 [03:34<12:49,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1922...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054e9897537c4fe1b1a2843ae5d5367e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████████████████▌                                                                                          | 22/100 [03:44<12:43,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 401...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c36616331dd4fc7ab05c25f9e387cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██████████████████████████▋                                                                                         | 23/100 [03:52<11:59,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2573...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2869c109a41f4a3089a9c757aa5595bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████████████▊                                                                                        | 24/100 [04:02<12:04,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2920...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c93329b00a4f6caaa727067d5cf79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████                                                                                       | 25/100 [04:12<11:58,  9.58s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1117...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfd8dac72fa42a58954591c4efc2a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████████████████████████▏                                                                                     | 26/100 [04:22<12:05,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1159...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77e8cc1b7cd46f696df8e6e32c0830e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████████████████████████▎                                                                                    | 27/100 [04:32<11:54,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 410...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c302af06112548808231fc2d5570133f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████████████████████████▍                                                                                   | 28/100 [04:43<12:07, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2403...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cde045aef77447b8b00d0f2a97d4284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|█████████████████████████████████▋                                                                                  | 29/100 [04:52<11:43,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1697...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c890bda143804313a3364bc46717fb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████▊                                                                                 | 30/100 [05:02<11:33,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 627...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da7c8fb3e8b48ae87453bed9a30df63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████▉                                                                                | 31/100 [05:13<11:38, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 104...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d96651d3d247feb86b8d902d01c5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████████████████                                                                               | 32/100 [05:22<11:16,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2237...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1914225696d42ba82ae4ac9e1857eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████████████▎                                                                             | 33/100 [05:33<11:19, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2995...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9e1b4772a14fe680073cb9d4291703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████████████████▍                                                                            | 34/100 [05:43<11:05, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2831...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0167f6d644f4bd185e545141fee6590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████████████████▌                                                                           | 35/100 [05:52<10:36,  9.79s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2493...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2663dfc93c7948d3b1cc5da4e15c9c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████████████████▊                                                                          | 36/100 [06:02<10:34,  9.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3048...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39572bed9ee64c859d0497108816e108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████████████████████████▉                                                                         | 37/100 [06:13<10:33, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1994...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12afdaf603a44350ad74fe2234aca750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████████████████████                                                                        | 38/100 [06:23<10:29, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1830...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270ffa7fa0854eb886f66a1503698303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████████████████████▏                                                                      | 39/100 [06:33<10:21, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1774...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e4fd3ebecc495fafda7c632010d66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████████████████████▍                                                                     | 40/100 [06:44<10:21, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70481c3a0fe04a2199c45d55d59afd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████████████████████▌                                                                    | 41/100 [06:53<09:53, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1780...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346dab1951e8403bbe7816739fd33054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████████████████████▋                                                                   | 42/100 [07:03<09:41, 10.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3003...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75f34b1dcbd4ca985581d9a7e81fb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████████████████████████████▉                                                                  | 43/100 [07:13<09:19,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 538...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bfe951193f4cc9b2c419cace1a89b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|███████████████████████████████████████████████████                                                                 | 44/100 [07:22<09:04,  9.72s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1976...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0062268fc64bb5b8815faffb7f74f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████████████████████▏                                                               | 45/100 [07:31<08:47,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1150...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6bcbdabc45469181abf277c744956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████████████████████▎                                                              | 46/100 [07:41<08:37,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1162...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02180dff592f411b8a448d4e9adff9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|██████████████████████████████████████████████████████▌                                                             | 47/100 [07:51<08:39,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 823...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2d2af3f4934a5eb5cfa77fe1b5267c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████████████████████▋                                                            | 48/100 [08:01<08:27,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2332...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61277ea8de9240f4a365537a39220143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████████████████████████████████████▊                                                           | 49/100 [08:11<08:14,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 657...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0916dadae2024dbe8f6bbfe94ec7a2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████████████████████                                                          | 50/100 [08:20<08:02,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 984...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0506fdcf803b40bd95479bed487fe58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████████████████████████▏                                                        | 51/100 [08:30<07:52,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a780227ac3d34c239cde9e731b84cf3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████████████████████████▎                                                       | 52/100 [08:40<07:52,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1750...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fc1ffc78dc457e97a16f07b2f18d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████████████████████████▍                                                      | 53/100 [08:50<07:49, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 520...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d79f27f5b64b8b890cf0e85d8888bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████████████████████████▋                                                     | 54/100 [09:00<07:39,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1887...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fe634b6ebb48cfb43048943933fd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████████████████████████████████████▊                                                    | 55/100 [09:10<07:29,  9.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1367...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f078e794ec3546909aca0f5970d8b3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████████████████████████▉                                                   | 56/100 [09:19<07:04,  9.65s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 603...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f8a760c7824ea681a11e02272a649f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████████████████████████                                                  | 57/100 [09:28<06:41,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2748...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e187cf807594a5c8a08ee7632aea706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████████████████████████▎                                                | 58/100 [09:38<06:40,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2273...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e737e688b94d84a7ed21735772e033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████████████████████████▍                                               | 59/100 [09:47<06:30,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 488...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6e2937a2b84d279bfd198f9be27e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████████████████████████▌                                              | 60/100 [09:57<06:28,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1815...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2883645f5a6f4d758eb981f0aa6a4ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████████████████████████████████████████████████████████████████████▊                                             | 61/100 [10:08<06:26,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1242...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d384bf98dbc2487f8fb14e9335083de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████████████▉                                            | 62/100 [10:19<06:36, 10.43s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2543...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe8b7d44e0c4d29a31603f7e99b76e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████████████████████████████████                                           | 63/100 [10:33<07:00, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 693...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0c3bfc70b8406595fadba384637697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████████████████████████████████████████████████▏                                         | 64/100 [10:43<06:37, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1512...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676198b64d6147868abdbfc050c98b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████████████████████████████████▍                                        | 65/100 [10:53<06:15, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 336...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059dc5d893e142408e0edcc80f00f084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████████████████████████████▌                                       | 66/100 [11:03<05:51, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3072...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e15ad5d56a45918a1c800603cdc814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████████████████████████████████████████████████████▋                                      | 67/100 [11:12<05:34, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1800...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67bf0973a7a4885bd449a6d98c22c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████████████████████████████▉                                     | 68/100 [11:24<05:34, 10.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1753...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57f53e0f7894bdea3406648baa3ab95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████████████                                    | 69/100 [11:34<05:23, 10.42s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 797...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52631f71949645d78646764e0de7cc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 70/100 [11:44<05:09, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2329...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f11e2392524d1fb3e7429f9350a8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████████████████████████████████▎                                 | 71/100 [11:53<04:46,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1162...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45dfaaeae1f4713be797eca81560ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████████████████████████████████▌                                | 72/100 [12:03<04:35,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1426...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31954b9f352d41b5b8405dfd99c51371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████████████████████▋                               | 73/100 [12:13<04:27,  9.92s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1007...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779bc7b7aca24247bff582a5e488b831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████▊                              | 74/100 [12:23<04:20, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 265...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e2e79cd3bb459e9bcf292bf4b69f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████████████████████████████                             | 75/100 [12:32<04:04,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 217...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3013b7a8819e4b77a8d1b06753a0d618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████▏                           | 76/100 [12:42<03:55,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2130...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9870b0bce3744b895896dd3ab72adb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████▎                          | 77/100 [12:51<03:41,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 586...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c057ca401944fcbb7d4555f233f0d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████▍                         | 78/100 [13:00<03:28,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3191...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dce9b3013d45b79001f1944acd1780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████▋                        | 79/100 [13:11<03:24,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1670...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5441973421a348868eed2f9f7a1256a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 80/100 [13:21<03:15,  9.75s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 321...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a31de08e4c4dfc86bcf09a29122813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████▉                      | 81/100 [13:30<03:02,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 886...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbbfe3c85c648d5a32b02dc91ac7564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████                     | 82/100 [13:41<03:01, 10.07s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2433...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a008f22daedc4766b5547d6c4307b197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 83/100 [13:51<02:50, 10.04s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2261...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe8f0af0a624d9ea21a04378a212566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                  | 84/100 [14:01<02:40, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 758...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe36cd2530040a584e1132bea6866b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 85/100 [19:22<25:50, 103.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 363...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc27a87f0cf640e1876d32b0cd1d4dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                | 86/100 [19:34<17:41, 75.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2294...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3f567754774392aa240e79e549a8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 87/100 [19:43<12:06, 55.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 577...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1333c78d4b934332808aba35a19673cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████              | 88/100 [19:54<08:29, 42.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2261...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63e5c4275c747e089c0895a8a1b630d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 89/100 [20:04<06:00, 32.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1114...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f3322c047948c9a735ce3c0b34cd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 90/100 [20:14<04:17, 25.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 642...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d78155c4c142cfa69e40469cafda86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 91/100 [20:23<03:06, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 232...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85f2687bf7c41d6a975e9a84c3ff32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 92/100 [20:33<02:19, 17.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1634...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f7e632fa4b4a90886ee2bcb36542f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 93/100 [20:42<01:45, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2038...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b1b78e7bc74ce9854c88273f5ddca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 94/100 [20:52<01:20, 13.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39600e5843cf46ff90775a643f38ebbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 95/100 [21:01<01:00, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 850...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1708a14a7d41d9ae6bd74fca123347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 96/100 [21:11<00:45, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2540...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be485c20e2e346f39df1bcbabac344d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 97/100 [21:21<00:33, 11.19s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 987...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41c336529544099885c6406990ef15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 98/100 [21:31<00:21, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2050...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143689e5031549c89d8dcff90b9bb80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 99/100 [21:41<00:10, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2344...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a4128a45c4d5297d50df29c3a9116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [21:51<00:00, 13.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP-Pretrained ViT-B/32 subspace score: 0.8943704742193223 (variance=0.0423109489549761)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                             | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2145...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136912e116a446b9bca2b6a7cf8d9dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▏                                                                                                                   | 1/100 [00:10<17:22, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2395...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95971df41604277a76bba6f761f72f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▎                                                                                                                  | 2/100 [00:20<16:46, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1373...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e728cb29bf7f4b789d00b5ac947766fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|███▌                                                                                                                 | 3/100 [00:30<16:00,  9.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1685...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765d73a2f4754b518f71a88d66c3fed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|████▋                                                                                                                | 4/100 [00:39<15:28,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2270...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c12ecbd5805460294632e4b5ca85e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████▊                                                                                                               | 5/100 [00:49<15:26,  9.75s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2496...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f214a38c429f4e039e929e92362c9db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███████                                                                                                              | 6/100 [00:59<15:19,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 556...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42569fd09593438e8c479583acd38b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|████████▏                                                                                                            | 7/100 [01:08<15:09,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1869...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b7f267efe3428990741676f521e011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████████▎                                                                                                           | 8/100 [01:19<15:09,  9.89s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1245...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78024dd95e3244e1bd2dd264e2708c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████████▌                                                                                                          | 9/100 [01:28<14:50,  9.79s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1800...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████▌                                                                                                        | 10/100 [01:30<10:59,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3152...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25321cc7fea54cb2878b2eaef967ed50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████████▍                                                                                                    | 11/100 [17:14<7:16:06, 294.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1599...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab42c15d68ea42b687487d7e06ec5b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████████▍                                                                                                  | 12/100 [34:08<12:32:40, 513.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1812...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f75f185f774477c9230127378d2e4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████████▌                                                                                                 | 13/100 [44:58<13:24:12, 554.63s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3027...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff00dcf3fff4321a9fea979b9942f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████████▊                                                                                                 | 14/100 [45:09<9:19:14, 390.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2665...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fece1f34958e4797bb5a9d84f4f0d622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████████▉                                                                                                | 15/100 [45:19<6:30:31, 275.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 140...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e78748297d424a9d42249ae5ddde2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████                                                                                               | 16/100 [45:28<4:33:39, 195.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 651...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b75fdd0195c44a6acac3ca7342e3289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████████▏                                                                                             | 17/100 [45:38<3:13:05, 139.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 396...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30aea1680f6415c860aac3d457f37b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████████████████▎                                                                                            | 18/100 [45:47<2:17:24, 100.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3057...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df4dc976e4547248c077375f5c79354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█████████████████████▋                                                                                            | 19/100 [45:58<1:39:14, 73.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1117...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca68fdddc094b0a9d27cafa8480b7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████▊                                                                                           | 20/100 [46:08<1:12:44, 54.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1908...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dd2a38afb94ac4b107afe30f541731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████████████████▎                                                                                           | 21/100 [46:17<53:48, 40.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1866...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d36fc926a3b48fc9a70ed54ba75fc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████████████▌                                                                                          | 22/100 [46:26<40:46, 31.37s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 8...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████████████▋                                                                                         | 23/100 [46:29<28:58, 22.57s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1604...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f391cb1bb9ad4bceaee6d4df65c9bc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|███████████████████████████▊                                                                                        | 24/100 [46:39<23:57, 18.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1322...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dce4c9dabb149e4b149df81825f27a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████████████                                                                                       | 25/100 [46:50<20:31, 16.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2166...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415a42cc87de496fab41ae17ed407826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████████████████████████▏                                                                                     | 26/100 [46:59<17:44, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1536...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071531218900498b927377dd8964ecfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████████████████████████▎                                                                                    | 27/100 [47:08<15:22, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1566...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5ac2075f094aaa81012e7d58353df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████████████████████████▍                                                                                   | 28/100 [47:18<14:23, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fa39801e624027ab0dd8ceb2d06c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|█████████████████████████████████▋                                                                                  | 29/100 [47:29<13:39, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1890...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b5c3df49014decb1bcfba5682202c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████▊                                                                                 | 30/100 [47:39<13:08, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1222...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cb701b6c0847beb828f50cfb9b022e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████▉                                                                                | 31/100 [47:50<12:37, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 912...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecb327dfc4240bb92cf60acbffd5e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████████████████                                                                               | 32/100 [47:59<11:51, 10.46s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2983...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b70e7cc0ce84c0b96fd2628b3e051a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████████████▎                                                                             | 33/100 [48:07<11:03,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 223...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4807c6326a544f6f944e1f8579453f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████████████████▍                                                                            | 34/100 [48:35<16:43, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1771...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab305cde16d45379af5f365ba2458d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████████████████▌                                                                           | 35/100 [48:44<14:35, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1893...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eda80bbea6b4a8dbd2ebff26078bd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████████████████████████▊                                                                          | 36/100 [48:55<13:32, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 752...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539fd44a2a334df89c9869e12dbed727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████████████████▉                                                                         | 37/100 [49:12<14:34, 13.87s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 158...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b11cb876eb44b8a4fdd0f81b20e406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████████████████████████████████                                                                        | 38/100 [49:22<13:09, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 996...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10a1138e7264cdd9e38ce34dddae829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|█████████████████████████████████████████████▏                                                                      | 39/100 [49:35<13:06, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3125...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031f154c1a4a4e90bf291630873d5dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████████████████████▍                                                                     | 40/100 [49:45<12:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3110...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356745af436b42f2859446ccc17445a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████████████████████▌                                                                    | 41/100 [49:55<11:00, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1031...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f5c302db9e476cae150ef3d74c7e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████████████████████▋                                                                   | 42/100 [50:04<10:14, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 884...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2ab18e617a4623bb39fb101ad906bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████████████████████████████▉                                                                  | 43/100 [50:13<09:49, 10.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1554...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9ca451b7094c7680de41cca0b7f579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|███████████████████████████████████████████████████                                                                 | 44/100 [50:23<09:21, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 134...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d3405a4ce5453dae5a54ea4529ef01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████████████████████████████████████████████████████▏                                                               | 45/100 [50:33<09:19, 10.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1964...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c33b72cb8a94eb994db085685b4a01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|█████████████████████████████████████████████████████▎                                                              | 46/100 [50:42<08:53,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 390...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81e8a2863244c3cac056e39afe8f398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████████████████████▌                                                             | 47/100 [50:51<08:28,  9.60s/it]WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 851...\n",
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8141cb87c1bd482ebf683a3d1e210bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████████████████████▋                                                            | 48/100 [51:00<08:10,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1815...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa245d5ef2cd4b58b9f82383e1882ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████████████████████████████████████▊                                                           | 49/100 [51:10<08:02,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 446...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0606b339ad8240759c351d2d2a7ec758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████████████████████                                                          | 50/100 [51:19<07:48,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1943...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c361926871a435382d73e2b05aacb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████████████████████████▏                                                        | 51/100 [51:28<07:35,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1947...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82dfe9fddd6468494140574f36648e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████████████████████████▎                                                       | 52/100 [51:37<07:19,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1922...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702087ca6e484a41a5827676ba1a4aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████████████████████████████████████████████████████████████▍                                                      | 53/100 [51:46<07:05,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1836...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d691d83352894719b3a1b802f90d130f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████████████████████████████████████████████▋                                                     | 54/100 [51:55<06:50,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2834...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c20a2f9b0e44f2b5318a9c7daeb628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████████████████████████████████████████████▊                                                    | 55/100 [52:03<06:39,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1180...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88463bc9040847728dbfb64c40e9177f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████████████████████████████▉                                                   | 56/100 [52:12<06:31,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2086...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbaf533575e4a28b893079ffc4d4828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████████████████████████                                                  | 57/100 [52:22<06:27,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 167...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f6edcaf121468c94893cbe563dc5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|███████████████████████████████████████████████████████████████████▎                                                | 58/100 [52:31<06:19,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2929...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c67412f41c4e119ec2ace7d7217635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████████████████████████▍                                               | 59/100 [52:40<06:10,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2605...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef7c0f6a0794a759ce045d05514bc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████████████████████████▌                                              | 60/100 [52:49<06:05,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1370...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e8f2b4a5234fa0a43132fbefdb496e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████████████████████████████████████████████████████████████████████▊                                             | 61/100 [52:59<06:01,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 259...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee5508ccb314b0398b0d49edbe6f651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████████████████████████▉                                            | 62/100 [53:08<05:54,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1294...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68ad2b4b10e4beea7fdaa66f270cbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████████████████████████████████████████                                           | 63/100 [53:17<05:40,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 491...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5479e3408f4c669be7864c83f59a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████████████████████████████████████████████████▏                                         | 64/100 [53:26<05:30,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 743...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a51b74268cc46cfb455f086e85ac4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████████████████████████████████▍                                        | 65/100 [53:35<05:18,  9.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2469...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8caacd60a2a459cbccce17bc62ae792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████████████████████████████▌                                       | 66/100 [53:44<05:04,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1521...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836955a9efb24b23b4ca0f69358792e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████████████████████████████████████████████████████▋                                      | 67/100 [53:53<04:55,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3087...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7158d2e144414e96bae07d5c3eb15898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████████████████████████████████████████████████████████████████████████████▉                                     | 68/100 [54:02<04:47,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2433...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cce32e12c9e462a9f8c562e6efa9c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████████████████████████████                                    | 69/100 [54:11<04:39,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2855...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f33d3f66bf45bf884a6718a5cb073b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████████████████████████████████████████████▏                                  | 70/100 [54:19<04:27,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1806...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c37efb291b041dc94e69d66a79f6485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████████████████████████████████████████████▎                                 | 71/100 [54:29<04:24,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2243...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████████████████████████████████████████████▌                                | 72/100 [54:31<03:17,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1355...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0749dc23e86a489ca435613387f40c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|████████████████████████████████████████████████████████████████████████████████████▋                               | 73/100 [54:40<03:25,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 648...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5ec9695dbd4bf598de79d31d401652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████▊                              | 74/100 [54:49<03:27,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2160...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a0003a381047f98ce84caf647dbab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████████████████████████████                             | 75/100 [54:58<03:27,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1269...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11493fc76fb43a6a4d864a7722c3be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████▏                           | 76/100 [55:08<03:29,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████████████████████████████████████████████████████████████▎                          | 77/100 [55:10<02:34,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2389...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd32920e83a4aed8294962d4d8ab4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████▍                         | 78/100 [55:19<02:45,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1557...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994f3d238226433fac52998f7f118417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████████████████████████████████████████████████▋                        | 79/100 [55:31<03:06,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 33...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9972470a4b34495aac1f008eeb53543b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████▊                       | 80/100 [55:41<03:01,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1177...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243dae4f5569439f933a53d6d6d32d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████▉                      | 81/100 [55:51<02:56,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1215...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d407029ab0424b9b9ff6c79c5d7b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████                     | 82/100 [56:00<02:47,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1913...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaedbb63603b4a58ab29215b5d7cf9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 83/100 [56:09<02:36,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2463...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0d1991f899408390c72899fb6d1336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                  | 84/100 [56:18<02:27,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2433...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29e600f9595484e93b8dacaa9c165a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                 | 85/100 [56:27<02:17,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60f13ac361349018c3629a71f5e6e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                | 86/100 [56:37<02:09,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1661...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50033e87dd747f890160b76c3de8d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 87/100 [56:46<01:59,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 96...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5747bc46fe70443f95f5b84ff1d83886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████              | 88/100 [56:55<01:50,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 3146...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dff72dbff82485cb2ebe159eb2a4eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 89/100 [57:04<01:42,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 802...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fe1a888c36496eb933190c4273c119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍           | 90/100 [57:14<01:33,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 446...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b9b0f1b67743fa94a590440314fdde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 91/100 [57:23<01:24,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 829...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d8f7242d6043859cf9d9ccae409baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 92/100 [57:33<01:15,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 857...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8e06da5ddf4a7bba9a899b8ce70cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 93/100 [57:42<01:05,  9.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 390...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a300138bcf3a493e9569299f8b8773a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 94/100 [57:51<00:56,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 514...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d85a7b9bff4c76a207c905a4bca521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 95/100 [58:01<00:46,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 2602...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbab6c7168f49389ab615f5b429f645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎    | 96/100 [58:10<00:37,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1941...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb75592534ca4b629c8b34bc1fe93620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 97/100 [58:19<00:27,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 1750...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcefa40d4cf49f299e182cf3aa48779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 98/100 [58:28<00:18,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 618...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a205cfa5396244d1a9b3d7f38bcf403d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊ | 99/100 [58:37<00:09,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for set 963...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d25fcc0c9f42aab4e1a1e72e12cbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [58:47<00:00, 35.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP-Pretrained ViT-B/32 subspace score: 0.805959764868021 (variance=0.2187121964389104)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in [\"scratch\", \"imagenet\", \"clip\"]:\n",
    "    for analysis in [\"shape\", \"texture\"]:\n",
    "        subspace_score(pretrain=model, n=100, mode=analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "5fe1ffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:00<00:00, 395.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP-Pretrained ViT-B/32 subspace score: 0.804276704788208 (variance=0.22334710229055582)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x900 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subspace_score(pretrain=\"clip\", sets=\"all\", mode=\"texture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37143487",
   "metadata": {},
   "source": [
    "# Adversarial shape/texture vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1a85648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3368fe991527413bac492e5646b6db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01683126389980316\n"
     ]
    }
   ],
   "source": [
    "def vector_derivative(model, base, same_feat_vecs, base_logits_diff, same_logits_diff, vec, criterion=None, layer=0, tok=0):\n",
    "    '''Computes the gradient on an intervention vector to change model decision.'''\n",
    "    \n",
    "    def patch_fn(\n",
    "        target_residual_component: Float[torch.Tensor, \"batch tok d_model\"],\n",
    "        hook,\n",
    "        tok,\n",
    "        same_vec,\n",
    "        diff_vec,\n",
    "    ):\n",
    "        target_residual_component[:, tok, :] += same_vec\n",
    "        target_residual_component[:, tok, :] -= diff_vec\n",
    "        return target_residual_component\n",
    "    \n",
    "    # Turn on grad\n",
    "    vec.requires_grad = True\n",
    "    vec.retain_grad()\n",
    "\n",
    "    #diff_vec = torch.sub(same_feat_vecs[layer, tok], vec)\n",
    "    \n",
    "    hook_fn = partial(patch_fn, tok=tok, same_vec=same_feat_vecs[layer, tok], diff_vec=vec)\n",
    "    \n",
    "    if not criterion:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.set_grad_enabled(True):\n",
    "        '''\n",
    "        # Set existing grad to zero\n",
    "        model.model.zero_grad()\n",
    "        \n",
    "        # Pass image through\n",
    "        out = model.model(im)\n",
    "        \n",
    "        # Get normalized attention scores & compute target\n",
    "        #attn1 = model.model.blocks[block].attn.attn_scores[0, head, source_tok, :]\n",
    "        #attn2 = model.model.blocks[block].attn.attn_scores[0, head, dest_tok, :]\n",
    "        #attn = attn1 + attn2\n",
    "        if isinstance(head, int):\n",
    "            attn = model.model.blocks[block].attn.attn_scores[0, head, source_tok, :]\n",
    "        else:\n",
    "            attn = model.model.blocks[block].attn.attn_scores[0, :, source_tok, :]\n",
    "            attn = torch.mean(attn, dim=0)\n",
    "            \n",
    "        targ = torch.zeros(attn.shape, requires_grad=True).to(\"mps\")\n",
    "        targ.data[dest_tok] = 1\n",
    "        #targ.data[source_tok] = 1\n",
    "        \n",
    "        # Compute loss between attention score & target & compute gradients\n",
    "        loss = criterion(attn, targ)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Retrieve gradient for image\n",
    "        im_grad = im.grad.data\n",
    "        return im_grad, loss.item(), attn\n",
    "        '''\n",
    "        model.zero_grad()\n",
    "        \n",
    "        patched_logits = model.run_with_hooks(\n",
    "            base,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_post\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logits_diff = patched_logits[0, 1] - patched_logits[0, 0]\n",
    "        patched_logits_diff = 2*(patched_logits_diff - base_logits_diff) / (same_logits_diff - base_logits_diff) - 1\n",
    "        \n",
    "        loss = (1 - patched_logits_diff)**2\n",
    "        loss.backward()\n",
    "        vec_grad = vec.grad.data\n",
    "        \n",
    "        return vec_grad, loss.item()\n",
    "    \n",
    "def run_adversarial_exp(set_idx, pretrain=\"imagenet\", mode=\"texture\", patch_size=32, device=\"mps\", restricted=True, \n",
    "                        eta=0.1, steps=50):\n",
    "    def preprocess(im):\n",
    "        if pretrain == \"clip\":\n",
    "            return image_processor(images=im, return_tensors='pt')[\"pixel_values\"].to(device)\n",
    "        else:\n",
    "            return image_processor.preprocess(im, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "    \n",
    "    num_tokens = (224 // patch_size)**2 + 1\n",
    "    \n",
    "    image_processor, model = load_model(pretrain=pretrain, patch_size=patch_size)\n",
    "    \n",
    "    set_dir = f\"stimuli/subspace/{mode}_{patch_size}/base_different/set_{set_idx}\"\n",
    "    base = preprocess(np.array(Image.open(os.path.join(set_dir, \"base.png\"))))\n",
    "    same = preprocess(np.array(Image.open(os.path.join(set_dir, \"same.png\"))))\n",
    "\n",
    "    base_logits, base_cache = model.run_with_cache(base, remove_batch_dim=True)\n",
    "    if base_logits[0, 0] < base_logits[0, 1]:\n",
    "        return\n",
    "\n",
    "    same_logits, same_cache = model.run_with_cache(same, remove_batch_dim=True)\n",
    "    if same_logits[0, 0] > same_logits[0, 1]:\n",
    "        return\n",
    "    \n",
    "    base_logits_diff = base_logits[0, 1] - base_logits[0, 0]\n",
    "    same_logits_diff = same_logits[0, 1] - same_logits[0, 0]\n",
    "    \n",
    "    # Images containing the original two different textures/shapes\n",
    "    base_feat = [np.array(Image.open(b), dtype=np.float32) for b in glob.glob(f\"{set_dir}/{mode}0_*.png\")]   \n",
    "    base_feat = preprocess(base_feat)\n",
    "    \n",
    "    # Images containing the same two textures\n",
    "    same_feat = [np.array(Image.open(s), dtype=np.float32) for s in glob.glob(f\"{set_dir}/{mode}1_*.png\")]\n",
    "    same_feat = preprocess(same_feat)\n",
    "    \n",
    "    def get_mean_cache(ims):\n",
    "        mean_cache = torch.zeros((model.cfg.n_layers, num_tokens, model.cfg.d_model), device=device)\n",
    "        _, cache = model.run_with_cache(ims)\n",
    "        \n",
    "        for layer in range(model.cfg.n_layers):\n",
    "            activations = cache[utils.get_act_name(\"resid_post\", layer)]\n",
    "            mean = torch.mean(activations, dim=0)\n",
    "            mean_cache[layer, :, :] = mean\n",
    "        \n",
    "        return mean_cache\n",
    "    \n",
    "    same_feat_vecs = get_mean_cache(same_feat)\n",
    "    base_feat_vecs = get_mean_cache(base_feat)\n",
    "\n",
    "    base2same_feat_vecs = same_feat_vecs - base_feat_vecs\n",
    "    \n",
    "    def patch_fn(\n",
    "        target_residual_component: Float[torch.Tensor, \"batch tok d_model\"],\n",
    "        hook,\n",
    "        tok,\n",
    "        diff_vec,\n",
    "    ):\n",
    "        target_residual_component[:, tok, :] += diff_vec\n",
    "        return target_residual_component\n",
    "    \n",
    "    logit_diffs = torch.zeros((model.cfg.n_layers, num_tokens), device=device)\n",
    "    \n",
    "    if restricted:\n",
    "        stim_dict = pickle.load(open(os.path.join(set_dir, \"datadict.pkl\"), \"rb\"))\n",
    "        \n",
    "        if stim_dict[\"base.png\"][\"t1\"] != stim_dict[\"same.png\"][\"t1\"]:  # Which object in the image is the edited one?\n",
    "            edited_pos = stim_dict[\"base.png\"][\"pos1\"] + 1\n",
    "            not_edited_pos = stim_dict[\"base.png\"][\"pos2\"] + 1\n",
    "        else:\n",
    "            edited_pos = stim_dict[\"base.png\"][\"pos2\"] + 1\n",
    "            not_edited_pos = stim_dict[\"base.png\"][\"pos1\"] + 1\n",
    "        \n",
    "        iterator = tqdm_itertools.product(range(model.cfg.n_layers), [0, edited_pos, not_edited_pos])\n",
    "        iterator = tqdm_itertools.product([2], [edited_pos])\n",
    "        restricted_str = \"_restricted\"\n",
    "    else:\n",
    "        iterator = tqdm_itertools.product(range(model.cfg.n_layers), range(num_tokens))\n",
    "        restricted_str = \"\"\n",
    "    \n",
    "    optim_vecs = {}\n",
    "    \n",
    "    for layer, tok in iterator:\n",
    "        vec = torch.from_numpy(np.random.normal(size=base2same_feat_vecs[layer, tok].shape).astype(np.float32)).to(device)\n",
    "        losses = []\n",
    "        \n",
    "        for i in range(steps):\n",
    "            d, loss = vector_derivative(model, base, same_feat_vecs, base_logits_diff, same_logits_diff, vec, layer=layer, tok=tok)\n",
    "            vec = vec.detach() - (eta * d)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        optim_vecs[f\"{layer}-{tok}\"] = vec\n",
    "        break\n",
    "        \n",
    "        '''\n",
    "        hook_fn = partial(patch_fn, tok=tok, diff_vec=base2same_feat_vecs[layer, tok])\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            base,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_post\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logits_diff = patched_logits[0, 1] - patched_logits[0, 0]\n",
    "        logit_diffs[layer, tok] = 2*(patched_logits_diff - base_logits_diff) / (same_logits_diff - base_logits_diff) - 1\n",
    "        '''\n",
    "\n",
    "    '''\n",
    "    torch.save(logit_diffs, f\"{set_dir}/logit_diffs_{pretrain}{restricted_str}{distractor_str}{random_str}.pt\", \n",
    "               pickle_protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    '''\n",
    "    print(losses[-1])\n",
    "    return optim_vecs, losses, base_feat_vecs\n",
    "\n",
    "'''\n",
    "def create_max_attn_im(model, im, steps, eta=0.1, head=11, block=3, source_tok=16, dest_tok=23, criterion=None,\n",
    "                      mask=False):\n",
    "\n",
    "    x = model.preprocess(im)\n",
    "    xs = [x]\n",
    "    losses = []\n",
    "    attns = []\n",
    "    \n",
    "    # Restrict gradient to dest_tok patch\n",
    "    if mask:\n",
    "        boolmask = torch.zeros(x.shape).to(\"mps\")\n",
    "        tok_boolmask(boolmask, dest_tok)\n",
    "    else:\n",
    "        boolmask = torch.ones(x.shape).to(\"mps\")\n",
    "\n",
    "    for i in tqdm.tqdm(range(steps)):\n",
    "        d, loss, attn = input_derivative(model, x, head=head, block=block, source_tok=source_tok, \n",
    "                                         dest_tok=dest_tok, criterion=criterion)\n",
    "\n",
    "        d = d * boolmask\n",
    "        \n",
    "        x = x.detach() - (eta * d)\n",
    "        #x[boolmask == 1] = torch.where(d[boolmask == 1] > 0, 1.0, 0.0).to(\"mps\")\n",
    "        #x.requires_grad = True\n",
    "        #x = torch.clamp(x, min=0)\n",
    "\n",
    "        xs.append(x)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if i == 0 or i == steps - 1:\n",
    "            attns.append(attn)\n",
    "\n",
    "    return xs, losses, attns\n",
    "    \n",
    "#im_grad, loss = input_derivative(model, test_im)\n",
    "def dist(x, y):\n",
    "    return torch.sqrt(torch.sum(torch.pow(torch.subtract(torch.flatten(x), torch.flatten(y)), 2), dim=0)) \n",
    "\n",
    "ims, losses, attns = create_max_attn_im(model, test_im, 5000, eta=0.01, head=\"all\", source_tok=13, dest_tok=12, mask=True)\n",
    "'''\n",
    "optim_vecs, losses, manual_vecs = run_adversarial_exp(349, pretrain=\"clip\", eta=0.1, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e99e2836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3050088e0>]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyOklEQVR4nO3de3xU9Z3/8feZmcwkIfeE3CBAUAQFBASBoPWy0iJSW7a7rut6odbany22Wvprlbba3e1a7HZ13e1SqfVn7dZ6qVVpi1aleKHWCHIJNxUvXBIuCdfcrzPz/f0xyZCBBDLJzJxM5vV8PM5jZs75npnPHC95P77f7/mOZYwxAgAAsInD7gIAAEBiI4wAAABbEUYAAICtCCMAAMBWhBEAAGArwggAALAVYQQAANiKMAIAAGzlsruAvvD7/Tpw4IDS09NlWZbd5QAAgD4wxqihoUHFxcVyOHrv/4iLMHLgwAGVlJTYXQYAAOiHqqoqjRw5stfjcRFG0tPTJQW+TEZGhs3VAACAvqivr1dJSUnw73hv4iKMdA3NZGRkEEYAAIgzZ5piwQRWAABgK8IIAACwFWEEAADYijACAABsRRgBAAC2IowAAABbEUYAAICtCCMAAMBWhBEAAGArwggAALAVYQQAANiKMAIAAGyV0GHkV2/v0dLnt2r3kSa7SwEAIGEldBh5YfN+PbW+Sjur6+0uBQCAhJXQYWRkdookad/xFpsrAQAgcSV4GEmVRBgBAMBOCR5GunpGmm2uBACAxEUYET0jAADYKaHDSEnOiWEaY4zN1QAAkJgSOoyMyAr0jDS2eVXX0mFzNQAAJKaEDiPJSU4NT/dIYqgGAAC7JHQYkZjECgCA3Qgj3N4LAICtCCPcUQMAgK0IIwzTAABgK8IIwzQAANiKMNJtmIa1RgAAiL2EDyPd1xqpbWatEQAAYi3hwwhrjQAAYK+EDyMSk1gBALBTWGFk2bJluvDCC5Wenq78/HwtXLhQO3fuPON5zz77rCZMmKDk5GRNnjxZL730Ur8LjgYmsQIAYJ+wwsibb76pxYsX65133tHq1avV0dGhz3zmM2pqaur1nLffflvXXXedbrnlFm3evFkLFy7UwoULtX379gEXHyn0jAAAYB/LDOAWksOHDys/P19vvvmmLrnkkh7bXHvttWpqatKqVauC+2bPnq2pU6dqxYoVffqc+vp6ZWZmqq6uThkZGf0tt1dPrqvUd1/Ypism5Ov/ffHCiL8/AACJqK9/vwc0Z6Surk6SlJOT02ub8vJyzZ07N2TfvHnzVF5e3us5bW1tqq+vD9miiVVYAQCwT7/DiN/v15133qmLLrpIkyZN6rVddXW1CgoKQvYVFBSourq613OWLVumzMzM4FZSUtLfMvuk+zANa40AABBb/Q4jixcv1vbt2/X0009Hsh5J0tKlS1VXVxfcqqqqIv4Z3RV3rjXS1O5jrREAAGLM1Z+Tbr/9dq1atUpr167VyJEjT9u2sLBQNTU1IftqampUWFjY6zkej0cej6c/pfVLcpJT+ekeHWpo077jLcoe5o7ZZwMAkOjC6hkxxuj222/XCy+8oNdee02lpaVnPKesrExr1qwJ2bd69WqVlZWFV2mUcUcNAAD2CCuMLF68WE888YSefPJJpaenq7q6WtXV1WppOTHx86abbtLSpUuDr++44w69/PLLeuCBB/TBBx/on//5n7VhwwbdfvvtkfsWEcBaIwAA2COsMPLwww+rrq5Ol112mYqKioLbM888E2xTWVmpgwcPBl/PmTNHTz75pB555BFNmTJFv/vd77Ry5crTTnq1Az0jAADYI6w5I3250+SNN944Zd8111yja665JpyPirmunpEqekYAAIgpfpumEz0jAADYgzDSqfvCZ6w1AgBA7BBGOnWtNdLc7tNx1hoBACBmCCOdutYakRiqAQAglggj3fAbNQAAxB5hpJuSnK61RugZAQAgVggj3dAzAgBA7BFGumEVVgAAYo8w0g1rjQAAEHuEkW6694yw1ggAALFBGOmmOCtZEmuNAAAQS4SRbjwupwoyWGsEAIBYIoycJPiDeceYxAoAQCwQRk7CJFYAAGKLMHIS1hoBACC2CCMnOXFHDT0jAADEAmHkJPSMAAAQW4SRk7DWCAAAsUUYOUlxVrIsS2rp8OlYU7vd5QAAMOQRRk7icTlVkB5Y/IyhGgAAoo8w0gPmjQAAEDuEkR6w1ggAALFDGOlB90msAAAguggjPaBnBACA2CGM9ICeEQAAYocw0oPuE1hZawQAgOgijPSgqNtaI0dZawQAgKgijPSAtUYAAIgdwkgvmMQKAEBsEEZ6wcJnAADEBmGkFyfuqKFnBACAaCKM9IKeEQAAYoMw0gvWGgEAIDYII70oyTkxgZW1RgAAiB7CSC+KMlNkWVJrh5+1RgAAiCLCSC/cLocKM1hrBACAaCOMnAZrjQAAEH2EkdNgEisAANFHGDmNrp6RqmP0jAAAEC2EkdNgrREAAKKPMHIarMIKAED0EUZOo3vPCGuNAAAQHYSR0+haa6TN69eRRtYaAQAgGggjpxG61ghDNQAARANh5AyYxAoAQHQRRs6AtUYAAIguwsgZlLAKKwAAUUUYOQN6RgAAiC7CyBnw+zQAAEQXYeQMuveMsNYIAACRRxg5g8LMZDlYawQAgKghjJwBa40AABBdhJE+6BqqqWISKwAAEUcY6QMmsQIAED2EkT5gFVYAAKKHMNIHrDUCAED0EEb6gGEaAACihzDSB109I/tZawQAgIgjjPRB97VGDje22V0OAABDCmGkD0LXGmHeCAAAkUQY6aOROUxiBQAgGggjfcQkVgAAooMw0kfc3gsAQHQQRvqIhc8AAIgOwkgfMUwDAEB0EEb6qKTbMI3fz1ojAABECmGkj7rWGmn3+nWEtUYAAIgYwkgfJTkdKsoMDNVUMW8EAICIIYyEYQTzRgAAiLiww8jatWt19dVXq7i4WJZlaeXKladt/8Ybb8iyrFO26urq/tZsG+6oAQAg8sIOI01NTZoyZYqWL18e1nk7d+7UwYMHg1t+fn64H2071hoBACDyXOGeMH/+fM2fPz/sD8rPz1dWVlbY5w0m3N4LAEDkxWzOyNSpU1VUVKRPf/rT+utf/3ratm1tbaqvrw/ZBoOuMLKfnhEAACIm6mGkqKhIK1as0HPPPafnnntOJSUluuyyy7Rp06Zez1m2bJkyMzODW0lJSbTL7JPgWiO1rDUCAECkWMaYfv9VtSxLL7zwghYuXBjWeZdeeqlGjRqlX//61z0eb2trU1vbibU86uvrVVJSorq6OmVkZPS33AHz+vwaf8/L8vmN1n/3CuVnJNtWCwAAg119fb0yMzPP+Pfbllt7Z86cqY8//rjX4x6PRxkZGSHbYOByOlTYGUBYawQAgMiwJYxUVFSoqKjIjo8eMCaxAgAQWWHfTdPY2BjSq7F7925VVFQoJydHo0aN0tKlS7V//3797//+ryTpoYceUmlpqSZOnKjW1lY9+uijeu211/Tqq69G7lvEUElOqtbtPqbKo4QRAAAiIewwsmHDBl1++eXB10uWLJEkLVq0SI8//rgOHjyoysrK4PH29nZ961vf0v79+5Wamqrzzz9ff/7zn0PeI56MzglMYt17jDACAEAkDGgCa6z0dQJMLPxhywF946nNunBMtp69bY6ttQAAMJgN6gms8SzYM8IwDQAAEUEYCdPo3EAYOdTQpuZ2r83VAAAQ/wgjYcpKdSszJUmSVMm8EQAABoww0g9dvSMM1QAAMHCEkX4YFZw30mRzJQAAxD/CSD+MyR0miZ4RAAAigTDSD6M6h2mYMwIAwMARRvqhq2dkD8M0AAAMGGGkH7omsB6obVWHz29zNQAAxDfCSD/kp3uUnOSQz2+0n1/vBQBgQAgj/WBZlkbnMFQDAEAkEEb6iUmsAABEBmGkn7p+o2bPEcIIAAADQRjpp9F5gWGaymMM0wAAMBCEkX7i13sBAIgMwkg/BVdhPdYsv9/YXA0AAPGLMNJPxVnJcjkstXv9qmlotbscAADiFmGkn1xOh0Zkp0hiqAYAgIEgjAzA6OAP5jGJFQCA/iKMDACTWAEAGDjCyAB0/UYNYQQAgP4jjAxAcJiGtUYAAOg3wsgABHtGjjTLGG7vBQCgPwgjAzAqJ1WWJTW0eXWsqd3ucgAAiEuEkQFITnKqODNwe++uIwzVAADQH4SRARo7PDBvZPdhwggAAP1BGBmg0s4fzKNnBACA/iGMDNDYzjCy+0ijzZUAABCfCCMDVDo8TZK0i2EaAAD6hTAyQF09I3uPNsvHr/cCABA2wsgAFWelyO1yqN3n14HaFrvLAQAg7hBGBsjpsDSmc/GzTw4zbwQAgHARRiKgNDiJlXkjAACEizASAWM7J7ESRgAACB9hJAKCa41wRw0AAGEjjETAWIZpAADoN8JIBHQN0+yvbVFrh8/magAAiC+EkQjITk1SZkqSJHpHAAAIF2EkAizL4o4aAAD6iTASIcFf7yWMAAAQFsJIhHRNYmXhMwAAwkMYiZDSPNYaAQCgPwgjEcIwDQAA/UMYiZAxuYEwUtvcoeNN7TZXAwBA/CCMREiK26kRWSmSpI+ZNwIAQJ8RRiLorPzAvJGPDxFGAADoK8JIBI3rDCMf1RBGAADoK8JIBAXDyKEGmysBACB+EEYiaFxBIIx8wjANAAB9RhiJoLOHp0uSDtS1qqG1w+ZqAACID4SRCMpMTdLwdI8k6ZPDrDcCAEBfEEYibBx31AAAEBbCSISdzSRWAADCQhiJsGDPCLf3AgDQJ4SRCDs7PzCJ9SOGaQAA6BPCSIR1DdNUHW9Wa4fP5moAABj8CCMRlpfmVlZqkoyRPuE3agAAOCPCSIRZlsUdNQAAhIEwEgVd80YIIwAAnBlhJArO5gfzAADoM8JIFHQN03zIWiMAAJwRYSQKxhcGhmn2HGnijhoAAM6AMBIF+ekeZaUmyW+YNwIAwJkQRqLAsixN6Owd+aCaoRoAAE6HMBIlEwozJEk7q+ttrgQAgMGNMBIl9IwAANA3hJEoGU8YAQCgTwgjUXJOQbosSzrc0KajjW12lwMAwKAVdhhZu3atrr76ahUXF8uyLK1cufKM57zxxhu64IIL5PF4dPbZZ+vxxx/vR6nxZZjHpVE5qZKknfSOAADQq7DDSFNTk6ZMmaLly5f3qf3u3bu1YMECXX755aqoqNCdd96pL3/5y3rllVfCLjbedM0beZ8wAgBAr1zhnjB//nzNnz+/z+1XrFih0tJSPfDAA5Kkc889V2+99Zb+8z//U/PmzQv34+PK+MIMvbKjhjtqAAA4jajPGSkvL9fcuXND9s2bN0/l5eW9ntPW1qb6+vqQLR6dyyRWAADOKOphpLq6WgUFBSH7CgoKVF9fr5aWlh7PWbZsmTIzM4NbSUlJtMuMiq47aj6saZDPb2yuBgCAwWlQ3k2zdOlS1dXVBbeqqiq7S+qX0bnDlJzkUGuHX3uPNtldDgAAg1LUw0hhYaFqampC9tXU1CgjI0MpKSk9nuPxeJSRkRGyxSOnw9I5BYHeEe6oAQCgZ1EPI2VlZVqzZk3IvtWrV6usrCzaHz0ocEcNAACnF3YYaWxsVEVFhSoqKiQFbt2tqKhQZWWlpMAQy0033RRsf9ttt2nXrl36zne+ow8++EA/+9nP9Nvf/lbf/OY3I/MNBrlziwK9Ou8diM9JuAAARFvYYWTDhg2aNm2apk2bJklasmSJpk2bpnvvvVeSdPDgwWAwkaTS0lK9+OKLWr16taZMmaIHHnhAjz766JC/rbfLpBGZkqQdB+psrgQAgMHJMsYM+ts86uvrlZmZqbq6uribP9LY5tXkf35Fxkgbvj9XeWkeu0sCACAm+vr3e1DeTTOUpHlcKs0bJknavp/eEQAATkYYiYFJxV1DNcwbAQDgZISRGJg0ItA1Rc8IAACnIozEQFfPyHYmsQIAcArCSAxM7AwjVcdaVNfcYXM1AAAMLoSRGMhMTVJJTmC1WW7xBQAgFGEkRiaPYKgGAICeEEZipGuoZtt+7qgBAKA7wkiMBFdi5Y4aAABCEEZiZGJx4PbeXUea1NDKJFYAALoQRmIkL82josxkSfxoHgAA3RFGYqhrEuvWfQzVAADQhTASQ1NHZUmSKqpqba0DAIDBhDASQ1NLsiQRRgAA6I4wEkPnj8ySZUn7a1t0qKHV7nIAABgUCCMxlOZx6Zz8dElSRWWtvcUAADBIEEZijKEaAABCEUZijEmsAACEIozEWFfPyNZ9dfL5jb3FAAAwCBBGYuycgnSlup1qbPPqk8ONdpcDAIDtCCMx5nRYwcXPmMQKAABhxBbTRmVLkjZVHre5EgAA7EcYscGM0YEw8u6eYzZXAgCA/QgjNpjeGUY+Odyko41tNlcDAIC9CCM2yB7m1rj8NEnSxr0M1QAAEhthxCYzxuRIkjYQRgAACY4wYpOZpcwbAQBAIozYZsboQM/Itn11amn32VwNAAD2IYzYZGR2igozkuX1G5aGBwAkNMKITSzL0owxgaGaDQzVAAASGGHERhd2TmJdTxgBACQwwoiNZpYGwsjGvcfV7vXbXA0AAPYgjNhofEG6coa51dzu09Z9tXaXAwCALQgjNnI4LJWNzZUklX9y1OZqAACwB2HEZrPPCoSRtwkjAIAERRix2ZzOMLKx8rhaO1hvBACQeAgjNhubN0wFGR61e/3axNLwAIAERBixmWVZmnNWniSGagAAiYkwMgiUBeeNHLG5EgAAYo8wMgh0zRvZsq9ODa0dNlcDAEBsEUYGgZHZqSrNGyaf3zBUAwBIOISRQeLSc4ZLkt7YedjmSgAAiC3CyCDRFUbWfnhYxhibqwEAIHYII4PE7LG5crsc2l/bok8ON9pdDgAAMUMYGSRS3E7N6vzhPIZqAACJhDAyiHQN1bz5IWEEAJA4CCODyGXj8yVJ63YdU3O71+ZqAACIDcLIIHLW8GEakZWidp+fX/EFACQMwsggYlmWrjg30Duy+r0am6sBACA2CCODzKfPK5Ak/fn9Gvn83OILABj6CCODzKzSXKUnu3SksV0VVfyKLwBg6COMDDJul0N/MyEwVPMqQzUAgARAGBmEuoZqXt1Rw2qsAIAhjzAyCF16znC5nQ7tPtLEaqwAgCGPMDIIpScnqeysXEnSKzsYqgEADG2EkUFq3sRCSdJL2w7aXAkAANFFGBmkrpxUKKfD0o4D9drFUA0AYAgjjAxSOcPcuvjsPEnSqq30jgAAhi7CyCD22fOLJEl/3HLA5koAAIgewsgg9pmJhXI7HfroUKN2VjfYXQ4AAFFBGBnEMlOSdMk5wyVJq7bSOwIAGJoII4Pc1VMCQzW/rzjAAmgAgCGJMDLIffq8AqV5XKo81qx39/BbNQCAoYcwMsilul1aMDnQO/K7jVU2VwMAQOQRRuLA388YKUl6cetBNbd7ba4GAIDIIozEgRmjszU6N1VN7T79aVu13eUAABBRhJE4YFmW/v6CQO/I7zbus7kaAAAiizASJ74wfaQsSyrfdVR7jzbZXQ4AABFDGIkTI7JSdGnnmiNPrqu0uRoAACKnX2Fk+fLlGjNmjJKTkzVr1iytX7++17aPP/64LMsK2ZKTk/tdcCK7YdZoSdIzG6rU2uGzuRoAACIj7DDyzDPPaMmSJfrBD36gTZs2acqUKZo3b54OHTrU6zkZGRk6ePBgcNu7d++Aik5Ul0/I14isFNU2d+hFfjwPADBEhB1GHnzwQd166626+eabdd5552nFihVKTU3VY4891us5lmWpsLAwuBUUFAyo6ETldFj6p1mjJEm/fodABwAYGsIKI+3t7dq4caPmzp174g0cDs2dO1fl5eW9ntfY2KjRo0erpKREn//857Vjx47Tfk5bW5vq6+tDNgRce2GJkpyWKqpqtW1fnd3lAAAwYGGFkSNHjsjn853Ss1FQUKDq6p7Xvxg/frwee+wx/f73v9cTTzwhv9+vOXPmaN++3m9RXbZsmTIzM4NbSUlJOGUOaXlpnuCKrI++tcvmagAAGLio301TVlamm266SVOnTtWll16q559/XsOHD9fPf/7zXs9ZunSp6urqgltVFcugd/flT42VJK3aelD7a1tsrgYAgIEJK4zk5eXJ6XSqpqYmZH9NTY0KCwv79B5JSUmaNm2aPv74417beDweZWRkhGw4YdKITM05K1c+v9Ev39ptdzkAAAxIWGHE7XZr+vTpWrNmTXCf3+/XmjVrVFZW1qf38Pl82rZtm4qKisKrFCG+ckmgd+Sp9ZWqa+mwuRoAAPov7GGaJUuW6Be/+IV+9atf6f3339dXv/pVNTU16eabb5Yk3XTTTVq6dGmw/b/+67/q1Vdf1a5du7Rp0ybdcMMN2rt3r7785S9H7lskoEvPGa7xBelqavfpN+u4swYAEL9c4Z5w7bXX6vDhw7r33ntVXV2tqVOn6uWXXw5Oaq2srJTDcSLjHD9+XLfeequqq6uVnZ2t6dOn6+2339Z5550XuW+RgCzL0lcuGatvPbtFj/5ltxaVjdEwT9j/OAEAsJ1ljDF2F3Em9fX1yszMVF1dHfNHuvH6/Jr74Jvac7RZd105QV+97Cy7SwIAIKivf7/5bZo45nI69PW/GSdJemTtJ2ps89pcEQAA4SOMxLnPTy1Wad4wHW/u0K/e3mN3OQAAhI0wEudcToe+ccXZkqRf/GWXGlq5swYAEF8II0PA1ecXa2zeMNU2d+iRtazKCgCIL4SRIcDldOg7V46XJD2ydpcOsCorACCOEEaGiHkTCzVzTI7avH79xys77S4HAIA+I4wMEZZl6fufPVeS9Pzm/dq6r9beggAA6CPCyBBy/sgsfWHaCEnSD1e9pzhYQgYAAMLIUPPtK8crOcmhd/cc1+827rO7HAAAzogwMsQUZabozrnnSJJ+9NL7OtbUbnNFAACcHmFkCLrl4lJNKEzX8eYO/duL79ldDgAAp0UYGYKSnA4t+8JkWZb0/Kb9+uvHR+wuCQCAXhFGhqhpo7J14+zRkqS7ntvKyqwAgEGLMDKEfXveeI3MTtG+4y364SqGawAAgxNhZAhLT07Sg/8wVZYl/XbDPr2yo9rukgAAOAVhZIibWZqj/3PJWZKkpc9v06GGVpsrAgAgFGEkAXzz0+N0blGGjjW16xtPbZbX57e7JAAAgggjCcDjcuqn103TMLdT7+w6pv949UO7SwIAIIgwkiDOzk/Tj//+fEnSijc/Yf4IAGDQIIwkkM+eX6wvXVQqSfq/v92ij2oabK4IAADCSMJZetUEzRyTo4Y2r25+/F0daWyzuyQAQIIjjCSYJKdDK26crtG5qdp3vEW3/u8GtXb47C4LAJDACCMJKGeYW4998UJlpiRpc2WtvvlMhXx+Y3dZAIAERRhJUGcNT9OKG6YryWnpT9urdddzW+UnkAAAbEAYSWBlZ+Xqp9dNk9Nh6Xcb9+lf/rhDxhBIAACxRRhJcFdOKtJ/XHO+LEv6Vfle/fjlnQQSAEBMEUagv502Uv+2cJKkwBok//LH9xiyAQDEDGEEkqTrZ43WDz8/UZL0+Nt79J3ntrJsPAAgJggjCLqxbIwe/IcpwTkkX39qM7f9AgCijjCCEF+4YKSW/9MFcjsd+tP2al33i3d0uIGF0QAA0UMYwSmunFSox790Yh2Shcv/qg+q6+0uCwAwRBFG0KM5Z+Xpha/NUWneMO2vbdHf/ext/WnbQbvLAgAMQYQR9Grs8DS98LU5Khubq6Z2n776m036lz/uULuXia0AgMghjOC0slLd+vUtM3XbpWdJkn751z36h5+Xq+pYs82VAQCGCsIIzsjldOju+RP06E0zlJHsUkVVra58aK2eWl/JAmkAgAEjjKDP5p5XoBe/8SnNHJOjpnaflj6/TV96/F3V1LfaXRoAII4RRhCWkpxUPfWV2freVefK7XLo9Z2H9ekH39Sv39nLL/8CAPqFMIKwOR2Wbr1krF78+sU6f2Sm6lu9umfldn3hZ3/Vtn11dpcHAIgzhBH027iCdL3wtYv0L5+bqHSPS1v21elzy9/S0ue36RBDNwCAPrJMHMxArK+vV2Zmpurq6pSRkWF3OejBoYZW3ffi+/p9xQFJUkqSU7deMlZfuWSs0jwum6sDANihr3+/CSOIqHf3HNOPXnpfmytrJUl5aW597bKzdd3MUUpxO+0tDgAQU4QR2MYYo5e3V+vfX9mp3UeaJEm5w9z68qfG6obZo5SenGRzhQCAWCCMwHYdPr+e3bBPD7/5saqOtUiSMlOS9MU5Y3TD7NEanu6xuUIAQDQRRjBoeH1+/WHLAS1//WN9cjjQU+J2OrTg/CJ9cc4YTSnJsrdAAEBUEEYw6Pj8geGbR9/aFZxTIklTS7J0w+zRumpyoVLdTHYFgKGCMIJBbUtVrX719h79cesBdfgC/woOczu14PwiXTOjRDNGZ8uyLJurBAAMBGEEceFwQ5ueebdSz27cp71HT/z43pjcVC2cNkILJhdpXEG6jRUCAPqLMIK4YozRu3uO69kNVXpx20E1t/uCx8blp2nB+UUEEwCIM4QRxK2mNq9e3l6tF7cd1F8+OhwcxpGks/PT9DcT8nX5+HzNGJOtJCeLCAPAYEUYwZBQ19KhP79X02MwSU926ZJxw3X5hHxdck6e8tOTbawUAHAywgiGnLqWDq398LBe/+CQ3vjwsI41tYccH5efprKzclU2Nlezx+Yqe5jbpkoBABJhBEOcz2+0ZV+tXv/gkF774JB2HKgPOW5Z0oTCDJWNzdX00dm6YHSWijJTbKoWABITYQQJ5XhTu9btPqq3Pzmq8k+O6qNDjae0KcpM1gWjsjVtVJYuGJ2ticUZ8rj4vRwAiBbCCBLaoYZWvbPrmNbvPqrNlbX6oLpBPn/ov+pup0PnFKZpYlGmJo7I0MTiDJ1blMHCawAQIYQRoJumNq+27qvTpsrj2lx5XJsqa0+ZcyIFhnfG5g3TxOJMnVecoXMK0jQuP10jslLkcLAIGwCEgzACnIYxRlXHWrTjQJ12HKgPPh5qaOuxfUqSU2fnp2lcfprOLkjTOfnpGleQppHZqXISUgCgR4QRoB8ON7QFg8n7B+v18aFG7TrcpHafv8f2SU5LJTmpGpM7TKNzQx9HZKewDgqAhEYYASLE6/Or8lizPjrUqI9qGjofG/XJ4Ua1eXsOKZLkdFgamZ2i0bnDVJKdouKsFI3MTtGIrMDzgoxkelUADGmEESDKfH6j6vpW7TnSpD1Hm7T3aLP2HOl8PNp02qAiBcJKYUayRmSnaGRnQBmRnaLCjGTlZ3iUn56s3GFu5qoAiFuEEcBGfr/RoYa2zpDSpP3HW7SvtkX7j7foQF2LDta2yus/8396Loel4eke5WckqyDdo/wMjwrSk1XQLbDkpbuVk+qWiyEhAINMX/9+cw8jEAUOh6XCzGQVZiZr9tjcU477/EaHG9q0v7ZZ+4636EBtq/bXNutAbatq6ltVU9+mo01t8vqNDta16mBd6xk/Mzs1STnD3MpN8yh3mFu5aW7lDPMoL80d2D/Mo9w0t3KHuZWV6maICMCgQRgBbODsFlamj+65jdfn15HG9s5w0qqahjYd7gwqNQ2Bx0P1rTrW3C5jpOPNHTre3KFPDjed8fMdlpSZkqSsVLcyU5I6nycpKyVJmaluZXW9Tk1SZor7xLGUJHpgAEQcYQQYpFxORzCwnI7Pb3S8uV3Hmtp1pLFNx5radbSxXUeb2nW0sU1HGzuPNQWO1TZ3yN8tvIQr3eNSRkqS0pNdnVuS0jwnnqcnu5SR7FJaskvpnqSQ/enJLqV5XAQaACEII0Ccczos5aV5lJfm0TkF6Wds3+Hz63hTu2pbOlTb3KHa5sDz+q7XLYHAUnfS64ZWrySpoc2rhjbvgGpOdTuDwWSYx6VUt1PD3C6lelwa5nYq1e3SMI9Tw056HfLodinVEzgvOckhy2LYCYhXhBEgwSQ5HcrPSFZ+xul7XE7m9flV3+pVbXO76loC4aSh1avGtsDz+lavGlo71Ni5v6HtRJuGzmNddxg1t/vU3O5TjXpeZC5clqVAOHE7lep2KjkpsKUkOZXidio5yXHiddcx94k2yUmOwKO72/GuY+4T57JuDBAdhBEAfeJyOpQzLDAZtr/avf5AYGk7EVJaOrxqbPOpuc2rpvZuj+1eNbV1Pvayv7ndJ0kyRmps86pxgD02Z+JyWPK4HPIkOeVxOeR2OQKvXc7g8572hTxPcsjtPPEenjDaJzktJTkc3O6NIYcwAiBm3C5H4G6fNE9E3s/vN2rp8Kmp3avmNp8a27xq7fCppcOn1g5/4LG963XgsaXDp7YOv1raT7xu7X68PXBu9/ZdCyB4/Ubedp+aOkOQXVwOS0lOh5KcltwuR+fzwOskZyAQdbU5cbzzdVdb10mvO/d1vXad3N5pKcnlCDnuclhyOXp57nTI6bCU5LQCj4QonAZhBEDccjiswLwSj0s683SZfjHGqN3nV2t7INy0eX1q8/rV7vUHnnf41ebzBx69vs79/tA2J7Vv796+83n7ad7j5J8j8PqNvH6fWsKff2wry5KSOgOLszMsBYKKJWdnr4+zM8gEAs2pIcfpcJwIOCHtuoefE226znE6JIfV9dqSo/OYwwq87tp62ufs9vrk81yd+5wnn9O1z3nSMcsilPWgX2Fk+fLl+slPfqLq6mpNmTJFP/3pTzVz5sxe2z/77LO65557tGfPHo0bN04//vGPddVVV/W7aACIFcuy5HE55XE5lakkW2rw+wOBqN3nl9dn1OELhJQOn18dXa99fnV4T3rdtXlN57mB4yHHfKbbe3U77j3p9Umf7fUbeX2dj92f+4y8fr96WtPPGHV+j9hfw8Gme4gJPu8h/JwcYrraOiwF9zmswER2h3UiJAUeAwGs63Mclro97/pMBc+75eJSleSk2nM9wj3hmWee0ZIlS7RixQrNmjVLDz30kObNm6edO3cqPz//lPZvv/22rrvuOi1btkyf/exn9eSTT2rhwoXatGmTJk2aFJEvAQBDmcNhKdkRmFgbL/xdIcUfGlK8PiOfPxBqAo+dr/3+kP2B9oGQ0+E38vn9wbahwad7KDqxv/t7BTdjQl/3sM9vAu/p7zzm9QX2dW/v90tev18+vwLtfYHw5fX75fcr+J6n4/Ub9ZjYbPS5qcW2hZGwl4OfNWuWLrzwQv3P//yPJMnv96ukpERf//rXdffdd5/S/tprr1VTU5NWrVoV3Dd79mxNnTpVK1as6NNnshw8ACCeGGPkN+pb6OkMbv6Twk/XvpMDVNe+wGMgEHVvZ8yJQOQ3XcEqEBD9pitQnXruDbNHqzgrJaLXISrLwbe3t2vjxo1aunRpcJ/D4dDcuXNVXl7e4znl5eVasmRJyL558+Zp5cqVvX5OW1ub2tpO3PJXX18fTpkAANjKsiw5O4dPcGZh3TR/5MgR+Xw+FRQUhOwvKChQdXV1j+dUV1eH1V6Sli1bpszMzOBWUlISTpkAACCODMoVfJYuXaq6urrgVlVVZXdJAAAgSsIapsnLy5PT6VRNTU3I/pqaGhUWFvZ4TmFhYVjtJcnj8cjjicw6BAAAYHALq2fE7XZr+vTpWrNmTXCf3+/XmjVrVFZW1uM5ZWVlIe0lafXq1b22BwAAiSXsW3uXLFmiRYsWacaMGZo5c6YeeughNTU16eabb5Yk3XTTTRoxYoSWLVsmSbrjjjt06aWX6oEHHtCCBQv09NNPa8OGDXrkkUci+00AAEBcCjuMXHvttTp8+LDuvfdeVVdXa+rUqXr55ZeDk1QrKyvlcJzocJkzZ46efPJJff/739d3v/tdjRs3TitXrmSNEQAAIKkf64zYgXVGAACIP339+z0o76YBAACJgzACAABsRRgBAAC2IowAAABbEUYAAICtCCMAAMBWYa8zYoeuu4/59V4AAOJH19/tM60iEhdhpKGhQZL49V4AAOJQQ0ODMjMzez0eF4ue+f1+HThwQOnp6bIsK2LvW19fr5KSElVVVbGYWpRxrWOD6xwbXOfY4DrHTrSutTFGDQ0NKi4uDlmd/WRx0TPicDg0cuTIqL1/RkYG/6LHCNc6NrjOscF1jg2uc+xE41qfrkekCxNYAQCArQgjAADAVgkdRjwej37wgx/I4/HYXcqQx7WODa5zbHCdY4PrHDt2X+u4mMAKAACGroTuGQEAAPYjjAAAAFsRRgAAgK0IIwAAwFYJHUaWL1+uMWPGKDk5WbNmzdL69evtLiluLFu2TBdeeKHS09OVn5+vhQsXaufOnSFtWltbtXjxYuXm5iotLU1/93d/p5qampA2lZWVWrBggVJTU5Wfn69vf/vb8nq9sfwqceX++++XZVm68847g/u4zpGzf/9+3XDDDcrNzVVKSoomT56sDRs2BI8bY3TvvfeqqKhIKSkpmjt3rj766KOQ9zh27Jiuv/56ZWRkKCsrS7fccosaGxtj/VUGLZ/Pp3vuuUelpaVKSUnRWWedpR/+8Ichv13Cde6ftWvX6uqrr1ZxcbEsy9LKlStDjkfqum7dulWf+tSnlJycrJKSEv37v//7wIs3Cerpp582brfbPPbYY2bHjh3m1ltvNVlZWaampsbu0uLCvHnzzC9/+Uuzfft2U1FRYa666iozatQo09jYGGxz2223mZKSErNmzRqzYcMGM3v2bDNnzpzgca/XayZNmmTmzp1rNm/ebF566SWTl5dnli5dasdXGvTWr19vxowZY84//3xzxx13BPdznSPj2LFjZvTo0eaLX/yiWbdundm1a5d55ZVXzMcffxxsc//995vMzEyzcuVKs2XLFvO5z33OlJaWmpaWlmCbK6+80kyZMsW888475i9/+Ys5++yzzXXXXWfHVxqU7rvvPpObm2tWrVpldu/ebZ599lmTlpZm/uu//ivYhuvcPy+99JL53ve+Z55//nkjybzwwgshxyNxXevq6kxBQYG5/vrrzfbt281TTz1lUlJSzM9//vMB1Z6wYWTmzJlm8eLFwdc+n88UFxebZcuW2VhV/Dp06JCRZN58801jjDG1tbUmKSnJPPvss8E277//vpFkysvLjTGB/3AcDoeprq4Otnn44YdNRkaGaWtri+0XGOQaGhrMuHHjzOrVq82ll14aDCNc58i56667zMUXX9zrcb/fbwoLC81PfvKT4L7a2lrj8XjMU089ZYwx5r333jOSzLvvvhts86c//clYlmX2798fveLjyIIFC8yXvvSlkH1f+MIXzPXXX2+M4TpHyslhJFLX9Wc/+5nJzs4O+X/HXXfdZcaPHz+gehNymKa9vV0bN27U3Llzg/scDofmzp2r8vJyGyuLX3V1dZKknJwcSdLGjRvV0dERco0nTJigUaNGBa9xeXm5Jk+erIKCgmCbefPmqb6+Xjt27Ihh9YPf4sWLtWDBgpDrKXGdI+kPf/iDZsyYoWuuuUb5+fmaNm2afvGLXwSP7969W9XV1SHXOjMzU7NmzQq51llZWZoxY0awzdy5c+VwOLRu3brYfZlBbM6cOVqzZo0+/PBDSdKWLVv01ltvaf78+ZK4ztESqetaXl6uSy65RG63O9hm3rx52rlzp44fP97v+uLih/Ii7ciRI/L5fCH/c5akgoICffDBBzZVFb/8fr/uvPNOXXTRRZo0aZIkqbq6Wm63W1lZWSFtCwoKVF1dHWzT0z+DrmMIePrpp7Vp0ya9++67pxzjOkfOrl279PDDD2vJkiX67ne/q3fffVff+MY35Ha7tWjRouC16uladr/W+fn5IcddLpdycnK41p3uvvtu1dfXa8KECXI6nfL5fLrvvvt0/fXXSxLXOUoidV2rq6tVWlp6ynt0HcvOzu5XfQkZRhBZixcv1vbt2/XWW2/ZXcqQU1VVpTvuuEOrV69WcnKy3eUMaX6/XzNmzNCPfvQjSdK0adO0fft2rVixQosWLbK5uqHjt7/9rX7zm9/oySef1MSJE1VRUaE777xTxcXFXOcElpDDNHl5eXI6nafccVBTU6PCwkKbqopPt99+u1atWqXXX39dI0eODO4vLCxUe3u7amtrQ9p3v8aFhYU9/jPoOobAMMyhQ4d0wQUXyOVyyeVy6c0339R///d/y+VyqaCggOscIUVFRTrvvPNC9p177rmqrKyUdOJane7/G4WFhTp06FDIca/Xq2PHjnGtO33729/W3XffrX/8x3/U5MmTdeONN+qb3/ymli1bJonrHC2Ruq7R+v9JQoYRt9ut6dOna82aNcF9fr9fa9asUVlZmY2VxQ9jjG6//Xa98MILeu21107ptps+fbqSkpJCrvHOnTtVWVkZvMZlZWXatm1byL/8q1evVkZGxil/FBLVFVdcoW3btqmioiK4zZgxQ9dff33wOdc5Mi666KJTbk//8MMPNXr0aElSaWmpCgsLQ651fX291q1bF3Kta2trtXHjxmCb1157TX6/X7NmzYrBtxj8mpub5XCE/ulxOp3y+/2SuM7REqnrWlZWprVr16qjoyPYZvXq1Ro/fny/h2gkJfatvR6Pxzz++OPmvffeM1/5yldMVlZWyB0H6N1Xv/pVk5mZad544w1z8ODB4Nbc3Bxsc9ttt5lRo0aZ1157zWzYsMGUlZWZsrKy4PGuW04/85nPmIqKCvPyyy+b4cOHc8vpGXS/m8YYrnOkrF+/3rhcLnPfffeZjz76yPzmN78xqamp5oknngi2uf/++01WVpb5/e9/b7Zu3Wo+//nP93hr5LRp08y6devMW2+9ZcaNG5fwt5x2t2jRIjNixIjgrb3PP/+8ycvLM9/5zneCbbjO/dPQ0GA2b95sNm/ebCSZBx980GzevNns3bvXGBOZ61pbW2sKCgrMjTfeaLZv326efvppk5qayq29A/HTn/7UjBo1yrjdbjNz5kzzzjvv2F1S3JDU4/bLX/4y2KalpcV87WtfM9nZ2SY1NdX87d/+rTl48GDI++zZs8fMnz/fpKSkmLy8PPOtb33LdHR0xPjbxJeTwwjXOXL++Mc/mkmTJhmPx2MmTJhgHnnkkZDjfr/f3HPPPaagoMB4PB5zxRVXmJ07d4a0OXr0qLnuuutMWlqaycjIMDfffLNpaGiI5dcY1Orr680dd9xhRo0aZZKTk83YsWPN9773vZBbRbnO/fP666/3+P/lRYsWGWMid123bNliLr74YuPxeMyIESPM/fffP+DaLWO6LXsHAAAQYwk5ZwQAAAwehBEAAGArwggAALAVYQQAANiKMAIAAGxFGAEAALYijAAAAFsRRgAAgK0IIwAAwFaEEQAAYCvCCAAAsBVhBAAA2Or/AztxTv0TnjocAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#optim_vecs['0-0'].shape\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7a0ac024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0337, device='mps:0')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_adv = optim_vecs['2-12']\n",
    "vec_manual = manual_vecs[2, 12]\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "cos(vec_adv, vec_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907289f2",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d4d9fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(pretrain=\"imagenet\", mode=\"texture\", patch_size=32):\n",
    "    try: \n",
    "        results = pickle.load(open(f\"stimuli/subspace/{mode}_{patch_size}/{pretrain}.pkl\", \"rb\"))\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            if torch.cuda.is_available():\n",
    "                device = torch.device(\"cuda\")\n",
    "            elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "                device = torch.device(\"mps\")\n",
    "            else:\n",
    "                device = torch.device(\"cpu\")\n",
    "        except AttributeError:  # if MPS is not available\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "        def preprocess(im):\n",
    "            if pretrain == \"clip\":\n",
    "                return image_processor(images=im, return_tensors='pt')[\"pixel_values\"].to(device)\n",
    "            else:\n",
    "                return image_processor.preprocess(im, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "        num_tokens = (224 // patch_size)**2 + 1\n",
    "\n",
    "        torch.set_grad_enabled(False)\n",
    "        image_processor, model = load_model(pretrain=pretrain, patch_size=patch_size)\n",
    "\n",
    "        sets = range(3200)\n",
    "        results = {i: {\"acc\": 0, \"same_acc\": 0, \"base_acc\": 0, \"same_logit\": 0, \"base_logit\": 0} for i in sets}\n",
    "\n",
    "        for set_idx in tqdm.tqdm(sets):\n",
    "            set_dir = f\"stimuli/subspace/{mode}_{patch_size}/base_different/set_{set_idx}\"\n",
    "\n",
    "            base = preprocess(np.array(Image.open(os.path.join(set_dir, \"base.png\"))))\n",
    "            same = preprocess(np.array(Image.open(os.path.join(set_dir, \"same.png\"))))\n",
    "\n",
    "            base_logit = model(base).squeeze(0)\n",
    "            same_logit = model(same).squeeze(0)\n",
    "\n",
    "            results[set_idx][\"base_logit\"] = base_logit\n",
    "            results[set_idx][\"same_logit\"] = same_logit\n",
    "\n",
    "            if base_logit[0] > base_logit[1]:\n",
    "                base_acc = True\n",
    "                results[set_idx][\"base_acc\"] += 1\n",
    "            else:\n",
    "                base_acc = False\n",
    "                \n",
    "            if same_logit[1] > same_logit[0]:\n",
    "                same_acc = True\n",
    "                results[set_idx][\"same_acc\"] += 1\n",
    "            else:\n",
    "                same_acc = False\n",
    "                \n",
    "            if same_acc and base_acc:\n",
    "                results[set_idx][\"acc\"] += 1\n",
    "\n",
    "        with open(f\"stimuli/subspace/{mode}_{patch_size}/{pretrain}.pkl\", \"wb\") as handle:\n",
    "            pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return results\n",
    "\n",
    "def get_test_accuracy(pretrain=\"imagenet\", mode=\"texture\", patch_size=32, compositional=192):\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "    except AttributeError:  # if MPS is not available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "    def preprocess(im):\n",
    "        if pretrain == \"clip\":\n",
    "            return image_processor(images=im, return_tensors='pt')[\"pixel_values\"].to(device)\n",
    "        else:\n",
    "            return image_processor.preprocess(im, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
    "\n",
    "    num_tokens = (224 // patch_size)**2 + 1\n",
    "\n",
    "    torch.set_grad_enabled(False)\n",
    "    image_processor, model = load_model(pretrain=pretrain, patch_size=patch_size, compositional=compositional)\n",
    "    \n",
    "    diff_imfiles = glob.glob(f\"stimuli/NOISE_RGB/aligned/N_32/trainsize_6400_{compositional}-{compositional}-{256-compositional}/test/different/*.png\")\n",
    "    same_imfiles = glob.glob(f\"stimuli/NOISE_RGB/aligned/N_32/trainsize_6400_{compositional}-{compositional}-{256-compositional}/test/same/*.png\")\n",
    "    \n",
    "    diff_ims = torch.zeros((len(diff_imfiles), 3, 224, 224)).to(device)\n",
    "    same_ims = torch.zeros((len(same_imfiles), 3, 224, 224)).to(device)\n",
    "    \n",
    "    for f in range(len(diff_imfiles)):\n",
    "        diff_ims[f, :, :, :] = preprocess(np.array(Image.open(diff_imfiles[f])))\n",
    "        \n",
    "    for f in range(len(same_imfiles)):\n",
    "        same_ims[f, :, :, :] = preprocess(np.array(Image.open(same_imfiles[f])))   \n",
    "\n",
    "    diff_logits = model(diff_ims)\n",
    "    same_logits = model(same_ims)\n",
    "\n",
    "    diff_acc = sum(diff_logits[:, 0] > diff_logits[:, 1]).item() / len(diff_imfiles)\n",
    "    same_acc = sum(same_logits[:, 0] < same_logits[:, 1]).item() / len(same_imfiles)\n",
    "    total_acc = (diff_acc + same_acc) / 2\n",
    "    \n",
    "    print(f\"{pretrain} compositional model {compositional}-{compositional}-{256-compositional}:\")\n",
    "    print(f\"\\tTotal accuracy: {total_acc}\")\n",
    "    print(f\"\\tDifferent accuracy: {diff_acc}\")\n",
    "    print(f\"\\tSame accuracy: {same_acc}\")\n",
    "    print()\n",
    "    #return diff_acc, same_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "aee89001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "imagenet compositional model 192-192-64:\n",
      "\tTotal accuracy: 0.985625\n",
      "\tDifferent accuracy: 0.976875\n",
      "\tSame accuracy: 0.994375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "imagenet compositional model 96-96-160:\n",
      "\tTotal accuracy: 0.971875\n",
      "\tDifferent accuracy: 0.9615625\n",
      "\tSame accuracy: 0.9821875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "imagenet compositional model 48-48-208:\n",
      "\tTotal accuracy: 0.9171875\n",
      "\tDifferent accuracy: 0.9275\n",
      "\tSame accuracy: 0.906875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "imagenet compositional model 32-32-224:\n",
      "\tTotal accuracy: 0.94421875\n",
      "\tDifferent accuracy: 0.9121875\n",
      "\tSame accuracy: 0.97625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ffdfc4764b4cd6974b8e80d448d8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip compositional model 192-192-64:\n",
      "\tTotal accuracy: 0.9993749999999999\n",
      "\tDifferent accuracy: 0.9990625\n",
      "\tSame accuracy: 0.9996875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n",
      "clip compositional model 96-96-160:\n",
      "\tTotal accuracy: 0.9942187499999999\n",
      "\tDifferent accuracy: 0.989375\n",
      "\tSame accuracy: 0.9990625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n",
      "clip compositional model 48-48-208:\n",
      "\tTotal accuracy: 0.98453125\n",
      "\tDifferent accuracy: 0.969375\n",
      "\tSame accuracy: 0.9996875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model openai/clip-vit-base-patch32 into HookedTransformer\n",
      "clip compositional model 32-32-224:\n",
      "\tTotal accuracy: 0.9840625000000001\n",
      "\tDifferent accuracy: 0.968125\n",
      "\tSame accuracy: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "scratch compositional model 192-192-64:\n",
      "\tTotal accuracy: 0.91171875\n",
      "\tDifferent accuracy: 0.879375\n",
      "\tSame accuracy: 0.9440625\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "scratch compositional model 96-96-160:\n",
      "\tTotal accuracy: 0.8807812500000001\n",
      "\tDifferent accuracy: 0.8678125\n",
      "\tSame accuracy: 0.89375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "scratch compositional model 48-48-208:\n",
      "\tTotal accuracy: 0.88546875\n",
      "\tDifferent accuracy: 0.8115625\n",
      "\tSame accuracy: 0.959375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Support for ViT is subject to the same caveats as in the BERT implementation.\n",
      "If using ViT for interpretability research, keep in mind that ViT has some significant architectural differences to GPT. For example, LayerNorms are applied *after* the attention and MLP components, meaning that the last LayerNorm in a block cannot be folded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n",
      "Loaded pretrained model google/vit-base-patch32-224-in21k into HookedTransformer\n",
      "Moving model to device:  mps\n",
      "scratch compositional model 32-32-224:\n",
      "\tTotal accuracy: 0.72921875\n",
      "\tDifferent accuracy: 0.8128125\n",
      "\tSame accuracy: 0.645625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pretrain in [\"imagenet\", \"clip\", \"scratch\"]:\n",
    "    for compositional in [192, 96, 48, 32]:\n",
    "        get_test_accuracy(pretrain=pretrain, compositional=compositional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504acbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "same_logit_diff = []\n",
    "same_same_logit = []\n",
    "same_diff_logit = []\n",
    "\n",
    "diff_logit_diff = []\n",
    "diff_same_logit = []\n",
    "diff_diff_logit = []\n",
    "\n",
    "for i in range(3200):\n",
    "    acc += results[i][\"acc\"]\n",
    "    same_logit_diff.append(abs(results[i][\"same_logit\"][1] - results[i][\"same_logit\"][0]).item())\n",
    "    same_same_logit.append(results[i][\"same_logit\"][1].item())\n",
    "    same_diff_logit.append(results[i][\"same_logit\"][0].item())\n",
    "    \n",
    "    diff_logit_diff.append(abs(results[i][\"base_logit\"][1] - results[i][\"base_logit\"][0]).item())\n",
    "    diff_same_logit.append(results[i][\"base_logit\"][1].item())\n",
    "    diff_diff_logit.append(results[i][\"base_logit\"][0].item())\n",
    "\n",
    "print(f\"Total accuracy: {(acc/3200)*100}%\")\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3), sharey=True)\n",
    "sns.histplot(data=same_logit_diff, ax=ax[0], color=\"#7B42FF\")\n",
    "ax[0].set_title('Logit Difference for \"Same\" Images')\n",
    "sns.histplot(data=same_diff_logit, ax=ax[1], color=\"#FF4242\")\n",
    "ax[1].set_title('\"Different\" Logit on \"Same\" Images')\n",
    "sns.histplot(data=same_same_logit, ax=ax[2], color=\"#42AFFF\")\n",
    "ax[2].set_title('\"Same\" Logit on \"Same\" Images')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3), sharey=True)\n",
    "sns.histplot(data=diff_logit_diff, ax=ax[0], color=\"#7B42FF\")\n",
    "ax[0].set_title('Logit Difference for \"Diff\" Images')\n",
    "sns.histplot(data=diff_diff_logit, ax=ax[1], color=\"#FF4242\")\n",
    "ax[1].set_title('\"Different\" Logit on \"Diff\" Images')\n",
    "sns.histplot(data=diff_same_logit, ax=ax[2], color=\"#42AFFF\")\n",
    "ax[2].set_title('\"Same\" Logit on \"Diff\" Images')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364d080",
   "metadata": {},
   "source": [
    "# Check compositional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a2c634e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compositional set 32-32-224:\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 32, 48, 96, 192\n",
    "for c in [32]:\n",
    "    train_data = pickle.load(open(f\"stimuli/NOISE_RGB/aligned/N_32/trainsize_6400_{c}-{c}-{256-c}/train/datadict.pkl\", \"rb\"))\n",
    "    val_data = pickle.load(open(f\"stimuli/NOISE_RGB/aligned/N_32/trainsize_6400_{c}-{c}-{256-c}/val/datadict.pkl\", \"rb\"))\n",
    "    test_data = pickle.load(open(f\"stimuli/NOISE_RGB/aligned/N_32/trainsize_6400_{c}-{c}-{256-c}/test/datadict.pkl\", \"rb\"))\n",
    "\n",
    "    train_combos = []\n",
    "    val_combos = []\n",
    "    test_combos = []\n",
    "\n",
    "    for t, v in train_data.items():\n",
    "        obj1 = f\"{v['s1']}-{v['t1']}\"\n",
    "        obj2 = f\"{v['s2']}-{v['t2']}\"\n",
    "        train_combos.append(obj1)\n",
    "        train_combos.append(obj2)\n",
    "    train_combos = set(train_combos)    \n",
    "\n",
    "    for t, v in val_data.items():\n",
    "        obj1 = f\"{v['s1']}-{v['t1']}\"\n",
    "        obj2 = f\"{v['s2']}-{v['t2']}\"\n",
    "        val_combos.append(obj1)\n",
    "        val_combos.append(obj2)\n",
    "    val_combos = set(val_combos)   \n",
    "\n",
    "    for t, v in test_data.items():\n",
    "        obj1 = f\"{v['s1']}-{v['t1']}\"\n",
    "        obj2 = f\"{v['s2']}-{v['t2']}\"\n",
    "        test_combos.append(obj1)\n",
    "        test_combos.append(obj2)\n",
    "    test_combos = set(test_combos)   \n",
    "\n",
    "    print(f\"Compositional set {c}-{c}-{256-c}:\")\n",
    "    print(len(train_combos) == c)\n",
    "    print(len(val_combos) == c)\n",
    "    print(len(test_combos) == 256-c)\n",
    "    print(len(train_combos.intersection(val_combos)) == c)\n",
    "    print(len(train_combos.intersection(test_combos)) == 0)\n",
    "    print(len(val_combos.intersection(test_combos)) == 0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55625e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
